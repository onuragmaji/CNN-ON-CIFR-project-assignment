{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "Uee4vFbzmYu_",
    "outputId": "69b1738f-1df0-4231-ba0e-94ff0ee8d472"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\santosh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\santosh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\santosh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\santosh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\santosh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\santosh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UqGmMJfkjxlN"
   },
   "outputs": [],
   "source": [
    "# import keras\n",
    "# from keras.datasets import cifar10\n",
    "# from keras.models import Model, Sequential\n",
    "# from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
    "# from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "# from keras.layers import Concatenate\n",
    "# from keras.optimizers import Adam\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nBT59LPCj4FY"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "l = 40\n",
    "num_filter = 12\n",
    "compression = 0.5\n",
    "dropout_rate = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "__VldHAskRNo"
   },
   "outputs": [],
   "source": [
    "# Load CIFAR10 Data\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "img_height, img_width, channel = X_train.shape[1],X_train.shape[2],X_train.shape[3]\n",
    "\n",
    "# convert to one hot encoing \n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "USoLa4_Lj4H0",
    "outputId": "10e5ed51-ed1e-4432-bad0-9300944bbd53"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standarding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v7u77XEXn3JG"
   },
   "outputs": [],
   "source": [
    "def prep_pixels(train, test):\n",
    "# convert from integers to floats\n",
    "    train_norm = train.astype('float32')\n",
    "    test_norm = test.astype('float32')\n",
    "# normalize to range 0-1\n",
    "    train_norm = train_norm / 255.0\n",
    "    test_norm = test_norm / 255.0\n",
    "# return normalized images\n",
    "    return train_norm, test_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pjip8_PuoTIG"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train,X_test=prep_pixels(X_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "Vzn95rmMj4KE",
    "outputId": "ba4fe7a3-5585-4c3d-d586-55660f2c36cd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/\n",
    "from numpy import expand_dims\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot\n",
    "c=X_train[1]\n",
    "c.shape\n",
    "samples = expand_dims(c, 0)\n",
    "# create image data augmentation generator\n",
    "datagen = ImageDataGenerator(rotation_range=90)\n",
    "# prepare iterator\n",
    "it = datagen.flow(samples, batch_size=1)\n",
    "# generate samples and plot\n",
    "for i in range(9):\n",
    "    pyplot.subplot(330 + 1 + i)\n",
    "# generate batch of images\n",
    "    batch = it.next()\n",
    "# convert to unsigned integers for viewing\n",
    "    image = batch[0].astype('uint8')\n",
    "# plot raw pixel data\n",
    "    pyplot.imshow(image)\n",
    "# show the figure\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KVYFMN2_ZXOv"
   },
   "source": [
    "# Defining the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Model using dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_ATDUK5vj4Mh"
   },
   "outputs": [],
   "source": [
    "# Dense Block\n",
    "def denseblock(input, num_filter = 64, dropout_rate = 0):\n",
    "    global compression\n",
    "    temp = input\n",
    "    for _ in range(l): \n",
    "        BatchNorm = layers.BatchNormalization()(temp)\n",
    "        relu = layers.Activation('relu')(BatchNorm)\n",
    "        Conv2D_3_3 = layers.Conv2D(int(num_filter*compression), (5,5),kernel_initializer=\"he_uniform\" ,padding='same')(relu)\n",
    "        if dropout_rate>0:\n",
    "            Conv2D_3_3 = layers.Dropout(dropout_rate)(Conv2D_3_3)\n",
    "        concat = layers.Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
    "        \n",
    "        temp = concat\n",
    "        \n",
    "    return temp\n",
    "\n",
    "## transition Blosck\n",
    "def transition(input, num_filter = 32, dropout_rate = 0):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    Conv2D_BottleNeck = layers.Conv2D(int(num_filter*compression), (5,5), kernel_initializer=\"he_uniform\" ,padding='same')(relu)\n",
    "    if dropout_rate>0:\n",
    "         Conv2D_BottleNeck = layers.Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
    "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
    "    return avg\n",
    "\n",
    "#output layer\n",
    "def output_layer(input):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    AvgPooling = layers.AveragePooling2D(pool_size=(2,2))(relu)\n",
    "    flat = layers.Flatten()(AvgPooling)\n",
    "    output = layers.Dense(num_classes, activation='softmax')(flat)\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0fZacgTQk_Gl"
   },
   "outputs": [],
   "source": [
    "num_filter = 10\n",
    "dropout_rate = 0\n",
    "l = 12\n",
    "input = layers.Input(shape=(img_height, img_width, channel,))\n",
    "First_Conv2D = layers.Conv2D(num_filter, (5,5), use_bias=False ,padding='same')(input)\n",
    "BatchNorm = layers.BatchNormalization()(First_Conv2D)\n",
    "\n",
    "First_Block = denseblock(BatchNorm,32, dropout_rate)\n",
    "First_Transition = transition(First_Block, num_filter, dropout_rate)\n",
    "\n",
    "Second_Block = denseblock(First_Transition, 16, dropout_rate)\n",
    "Second_Transition = transition(Second_Block, num_filter, dropout_rate)\n",
    "\n",
    "Third_Block = denseblock(Second_Transition, num_filter, dropout_rate)\n",
    "Third_Transition = transition(Third_Block, num_filter, dropout_rate)\n",
    "\n",
    "Last_Block = denseblock(Third_Transition,  num_filter, dropout_rate)\n",
    "output = output_layer(Last_Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "voLWzMSFj4Oy",
    "outputId": "27d78ba3-6578-4728-df98-c7ca85c19ff7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_365 (Conv2D)             (None, 32, 32, 10)   750         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_373 (BatchN (None, 32, 32, 10)   40          conv2d_365[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_374 (BatchN (None, 32, 32, 10)   40          batch_normalization_373[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_365 (Activation)     (None, 32, 32, 10)   0           batch_normalization_374[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_366 (Conv2D)             (None, 32, 32, 16)   4016        activation_365[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_336 (Concatenate)   (None, 32, 32, 26)   0           batch_normalization_373[0][0]    \n",
      "                                                                 conv2d_366[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_375 (BatchN (None, 32, 32, 26)   104         concatenate_336[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_366 (Activation)     (None, 32, 32, 26)   0           batch_normalization_375[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_367 (Conv2D)             (None, 32, 32, 16)   10416       activation_366[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_337 (Concatenate)   (None, 32, 32, 42)   0           concatenate_336[0][0]            \n",
      "                                                                 conv2d_367[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_376 (BatchN (None, 32, 32, 42)   168         concatenate_337[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_367 (Activation)     (None, 32, 32, 42)   0           batch_normalization_376[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_368 (Conv2D)             (None, 32, 32, 16)   16816       activation_367[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_338 (Concatenate)   (None, 32, 32, 58)   0           concatenate_337[0][0]            \n",
      "                                                                 conv2d_368[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_377 (BatchN (None, 32, 32, 58)   232         concatenate_338[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_368 (Activation)     (None, 32, 32, 58)   0           batch_normalization_377[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_369 (Conv2D)             (None, 32, 32, 16)   23216       activation_368[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_339 (Concatenate)   (None, 32, 32, 74)   0           concatenate_338[0][0]            \n",
      "                                                                 conv2d_369[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_378 (BatchN (None, 32, 32, 74)   296         concatenate_339[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_369 (Activation)     (None, 32, 32, 74)   0           batch_normalization_378[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_370 (Conv2D)             (None, 32, 32, 16)   29616       activation_369[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_340 (Concatenate)   (None, 32, 32, 90)   0           concatenate_339[0][0]            \n",
      "                                                                 conv2d_370[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_379 (BatchN (None, 32, 32, 90)   360         concatenate_340[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_370 (Activation)     (None, 32, 32, 90)   0           batch_normalization_379[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_371 (Conv2D)             (None, 32, 32, 16)   36016       activation_370[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_341 (Concatenate)   (None, 32, 32, 106)  0           concatenate_340[0][0]            \n",
      "                                                                 conv2d_371[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_380 (BatchN (None, 32, 32, 106)  424         concatenate_341[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_371 (Activation)     (None, 32, 32, 106)  0           batch_normalization_380[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_372 (Conv2D)             (None, 32, 32, 16)   42416       activation_371[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_342 (Concatenate)   (None, 32, 32, 122)  0           concatenate_341[0][0]            \n",
      "                                                                 conv2d_372[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_381 (BatchN (None, 32, 32, 122)  488         concatenate_342[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_372 (Activation)     (None, 32, 32, 122)  0           batch_normalization_381[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_373 (Conv2D)             (None, 32, 32, 16)   48816       activation_372[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_343 (Concatenate)   (None, 32, 32, 138)  0           concatenate_342[0][0]            \n",
      "                                                                 conv2d_373[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_382 (BatchN (None, 32, 32, 138)  552         concatenate_343[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_373 (Activation)     (None, 32, 32, 138)  0           batch_normalization_382[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_374 (Conv2D)             (None, 32, 32, 16)   55216       activation_373[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_344 (Concatenate)   (None, 32, 32, 154)  0           concatenate_343[0][0]            \n",
      "                                                                 conv2d_374[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_383 (BatchN (None, 32, 32, 154)  616         concatenate_344[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_374 (Activation)     (None, 32, 32, 154)  0           batch_normalization_383[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_375 (Conv2D)             (None, 32, 32, 16)   61616       activation_374[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_345 (Concatenate)   (None, 32, 32, 170)  0           concatenate_344[0][0]            \n",
      "                                                                 conv2d_375[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_384 (BatchN (None, 32, 32, 170)  680         concatenate_345[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_375 (Activation)     (None, 32, 32, 170)  0           batch_normalization_384[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_376 (Conv2D)             (None, 32, 32, 16)   68016       activation_375[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_346 (Concatenate)   (None, 32, 32, 186)  0           concatenate_345[0][0]            \n",
      "                                                                 conv2d_376[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_385 (BatchN (None, 32, 32, 186)  744         concatenate_346[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_376 (Activation)     (None, 32, 32, 186)  0           batch_normalization_385[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_377 (Conv2D)             (None, 32, 32, 16)   74416       activation_376[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_347 (Concatenate)   (None, 32, 32, 202)  0           concatenate_346[0][0]            \n",
      "                                                                 conv2d_377[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_386 (BatchN (None, 32, 32, 202)  808         concatenate_347[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_377 (Activation)     (None, 32, 32, 202)  0           batch_normalization_386[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_378 (Conv2D)             (None, 32, 32, 5)    25255       activation_377[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_28 (AveragePo (None, 16, 16, 5)    0           conv2d_378[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_387 (BatchN (None, 16, 16, 5)    20          average_pooling2d_28[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_378 (Activation)     (None, 16, 16, 5)    0           batch_normalization_387[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_379 (Conv2D)             (None, 16, 16, 8)    1008        activation_378[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_348 (Concatenate)   (None, 16, 16, 13)   0           average_pooling2d_28[0][0]       \n",
      "                                                                 conv2d_379[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_388 (BatchN (None, 16, 16, 13)   52          concatenate_348[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_379 (Activation)     (None, 16, 16, 13)   0           batch_normalization_388[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_380 (Conv2D)             (None, 16, 16, 8)    2608        activation_379[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_349 (Concatenate)   (None, 16, 16, 21)   0           concatenate_348[0][0]            \n",
      "                                                                 conv2d_380[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_389 (BatchN (None, 16, 16, 21)   84          concatenate_349[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_380 (Activation)     (None, 16, 16, 21)   0           batch_normalization_389[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_381 (Conv2D)             (None, 16, 16, 8)    4208        activation_380[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_350 (Concatenate)   (None, 16, 16, 29)   0           concatenate_349[0][0]            \n",
      "                                                                 conv2d_381[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_390 (BatchN (None, 16, 16, 29)   116         concatenate_350[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_381 (Activation)     (None, 16, 16, 29)   0           batch_normalization_390[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_382 (Conv2D)             (None, 16, 16, 8)    5808        activation_381[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_351 (Concatenate)   (None, 16, 16, 37)   0           concatenate_350[0][0]            \n",
      "                                                                 conv2d_382[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_391 (BatchN (None, 16, 16, 37)   148         concatenate_351[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_382 (Activation)     (None, 16, 16, 37)   0           batch_normalization_391[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_383 (Conv2D)             (None, 16, 16, 8)    7408        activation_382[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_352 (Concatenate)   (None, 16, 16, 45)   0           concatenate_351[0][0]            \n",
      "                                                                 conv2d_383[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_392 (BatchN (None, 16, 16, 45)   180         concatenate_352[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_383 (Activation)     (None, 16, 16, 45)   0           batch_normalization_392[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_384 (Conv2D)             (None, 16, 16, 8)    9008        activation_383[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_353 (Concatenate)   (None, 16, 16, 53)   0           concatenate_352[0][0]            \n",
      "                                                                 conv2d_384[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_393 (BatchN (None, 16, 16, 53)   212         concatenate_353[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_384 (Activation)     (None, 16, 16, 53)   0           batch_normalization_393[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_385 (Conv2D)             (None, 16, 16, 8)    10608       activation_384[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_354 (Concatenate)   (None, 16, 16, 61)   0           concatenate_353[0][0]            \n",
      "                                                                 conv2d_385[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_394 (BatchN (None, 16, 16, 61)   244         concatenate_354[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_385 (Activation)     (None, 16, 16, 61)   0           batch_normalization_394[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_386 (Conv2D)             (None, 16, 16, 8)    12208       activation_385[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_355 (Concatenate)   (None, 16, 16, 69)   0           concatenate_354[0][0]            \n",
      "                                                                 conv2d_386[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_395 (BatchN (None, 16, 16, 69)   276         concatenate_355[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_386 (Activation)     (None, 16, 16, 69)   0           batch_normalization_395[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_387 (Conv2D)             (None, 16, 16, 8)    13808       activation_386[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_356 (Concatenate)   (None, 16, 16, 77)   0           concatenate_355[0][0]            \n",
      "                                                                 conv2d_387[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_396 (BatchN (None, 16, 16, 77)   308         concatenate_356[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_387 (Activation)     (None, 16, 16, 77)   0           batch_normalization_396[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_388 (Conv2D)             (None, 16, 16, 8)    15408       activation_387[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_357 (Concatenate)   (None, 16, 16, 85)   0           concatenate_356[0][0]            \n",
      "                                                                 conv2d_388[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_397 (BatchN (None, 16, 16, 85)   340         concatenate_357[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_388 (Activation)     (None, 16, 16, 85)   0           batch_normalization_397[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_389 (Conv2D)             (None, 16, 16, 8)    17008       activation_388[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_358 (Concatenate)   (None, 16, 16, 93)   0           concatenate_357[0][0]            \n",
      "                                                                 conv2d_389[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_398 (BatchN (None, 16, 16, 93)   372         concatenate_358[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_389 (Activation)     (None, 16, 16, 93)   0           batch_normalization_398[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_390 (Conv2D)             (None, 16, 16, 8)    18608       activation_389[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_359 (Concatenate)   (None, 16, 16, 101)  0           concatenate_358[0][0]            \n",
      "                                                                 conv2d_390[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_399 (BatchN (None, 16, 16, 101)  404         concatenate_359[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_390 (Activation)     (None, 16, 16, 101)  0           batch_normalization_399[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_391 (Conv2D)             (None, 16, 16, 5)    12630       activation_390[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_29 (AveragePo (None, 8, 8, 5)      0           conv2d_391[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_400 (BatchN (None, 8, 8, 5)      20          average_pooling2d_29[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_391 (Activation)     (None, 8, 8, 5)      0           batch_normalization_400[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_392 (Conv2D)             (None, 8, 8, 5)      630         activation_391[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_360 (Concatenate)   (None, 8, 8, 10)     0           average_pooling2d_29[0][0]       \n",
      "                                                                 conv2d_392[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_401 (BatchN (None, 8, 8, 10)     40          concatenate_360[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_392 (Activation)     (None, 8, 8, 10)     0           batch_normalization_401[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_393 (Conv2D)             (None, 8, 8, 5)      1255        activation_392[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_361 (Concatenate)   (None, 8, 8, 15)     0           concatenate_360[0][0]            \n",
      "                                                                 conv2d_393[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_402 (BatchN (None, 8, 8, 15)     60          concatenate_361[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_393 (Activation)     (None, 8, 8, 15)     0           batch_normalization_402[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_394 (Conv2D)             (None, 8, 8, 5)      1880        activation_393[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_362 (Concatenate)   (None, 8, 8, 20)     0           concatenate_361[0][0]            \n",
      "                                                                 conv2d_394[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_403 (BatchN (None, 8, 8, 20)     80          concatenate_362[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_394 (Activation)     (None, 8, 8, 20)     0           batch_normalization_403[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_395 (Conv2D)             (None, 8, 8, 5)      2505        activation_394[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_363 (Concatenate)   (None, 8, 8, 25)     0           concatenate_362[0][0]            \n",
      "                                                                 conv2d_395[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_404 (BatchN (None, 8, 8, 25)     100         concatenate_363[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_395 (Activation)     (None, 8, 8, 25)     0           batch_normalization_404[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_396 (Conv2D)             (None, 8, 8, 5)      3130        activation_395[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_364 (Concatenate)   (None, 8, 8, 30)     0           concatenate_363[0][0]            \n",
      "                                                                 conv2d_396[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_405 (BatchN (None, 8, 8, 30)     120         concatenate_364[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_396 (Activation)     (None, 8, 8, 30)     0           batch_normalization_405[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_397 (Conv2D)             (None, 8, 8, 5)      3755        activation_396[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_365 (Concatenate)   (None, 8, 8, 35)     0           concatenate_364[0][0]            \n",
      "                                                                 conv2d_397[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_406 (BatchN (None, 8, 8, 35)     140         concatenate_365[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_397 (Activation)     (None, 8, 8, 35)     0           batch_normalization_406[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_398 (Conv2D)             (None, 8, 8, 5)      4380        activation_397[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_366 (Concatenate)   (None, 8, 8, 40)     0           concatenate_365[0][0]            \n",
      "                                                                 conv2d_398[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_407 (BatchN (None, 8, 8, 40)     160         concatenate_366[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_398 (Activation)     (None, 8, 8, 40)     0           batch_normalization_407[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_399 (Conv2D)             (None, 8, 8, 5)      5005        activation_398[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_367 (Concatenate)   (None, 8, 8, 45)     0           concatenate_366[0][0]            \n",
      "                                                                 conv2d_399[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_408 (BatchN (None, 8, 8, 45)     180         concatenate_367[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_399 (Activation)     (None, 8, 8, 45)     0           batch_normalization_408[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_400 (Conv2D)             (None, 8, 8, 5)      5630        activation_399[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_368 (Concatenate)   (None, 8, 8, 50)     0           concatenate_367[0][0]            \n",
      "                                                                 conv2d_400[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_409 (BatchN (None, 8, 8, 50)     200         concatenate_368[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_400 (Activation)     (None, 8, 8, 50)     0           batch_normalization_409[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_401 (Conv2D)             (None, 8, 8, 5)      6255        activation_400[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_369 (Concatenate)   (None, 8, 8, 55)     0           concatenate_368[0][0]            \n",
      "                                                                 conv2d_401[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_410 (BatchN (None, 8, 8, 55)     220         concatenate_369[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_401 (Activation)     (None, 8, 8, 55)     0           batch_normalization_410[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_402 (Conv2D)             (None, 8, 8, 5)      6880        activation_401[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_370 (Concatenate)   (None, 8, 8, 60)     0           concatenate_369[0][0]            \n",
      "                                                                 conv2d_402[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_411 (BatchN (None, 8, 8, 60)     240         concatenate_370[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_402 (Activation)     (None, 8, 8, 60)     0           batch_normalization_411[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_403 (Conv2D)             (None, 8, 8, 5)      7505        activation_402[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_371 (Concatenate)   (None, 8, 8, 65)     0           concatenate_370[0][0]            \n",
      "                                                                 conv2d_403[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_412 (BatchN (None, 8, 8, 65)     260         concatenate_371[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_403 (Activation)     (None, 8, 8, 65)     0           batch_normalization_412[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_404 (Conv2D)             (None, 8, 8, 5)      8130        activation_403[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_30 (AveragePo (None, 4, 4, 5)      0           conv2d_404[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_413 (BatchN (None, 4, 4, 5)      20          average_pooling2d_30[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_404 (Activation)     (None, 4, 4, 5)      0           batch_normalization_413[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_405 (Conv2D)             (None, 4, 4, 5)      630         activation_404[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_372 (Concatenate)   (None, 4, 4, 10)     0           average_pooling2d_30[0][0]       \n",
      "                                                                 conv2d_405[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_414 (BatchN (None, 4, 4, 10)     40          concatenate_372[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_405 (Activation)     (None, 4, 4, 10)     0           batch_normalization_414[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_406 (Conv2D)             (None, 4, 4, 5)      1255        activation_405[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_373 (Concatenate)   (None, 4, 4, 15)     0           concatenate_372[0][0]            \n",
      "                                                                 conv2d_406[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_415 (BatchN (None, 4, 4, 15)     60          concatenate_373[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_406 (Activation)     (None, 4, 4, 15)     0           batch_normalization_415[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_407 (Conv2D)             (None, 4, 4, 5)      1880        activation_406[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_374 (Concatenate)   (None, 4, 4, 20)     0           concatenate_373[0][0]            \n",
      "                                                                 conv2d_407[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_416 (BatchN (None, 4, 4, 20)     80          concatenate_374[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_407 (Activation)     (None, 4, 4, 20)     0           batch_normalization_416[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_408 (Conv2D)             (None, 4, 4, 5)      2505        activation_407[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_375 (Concatenate)   (None, 4, 4, 25)     0           concatenate_374[0][0]            \n",
      "                                                                 conv2d_408[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_417 (BatchN (None, 4, 4, 25)     100         concatenate_375[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_408 (Activation)     (None, 4, 4, 25)     0           batch_normalization_417[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_409 (Conv2D)             (None, 4, 4, 5)      3130        activation_408[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_376 (Concatenate)   (None, 4, 4, 30)     0           concatenate_375[0][0]            \n",
      "                                                                 conv2d_409[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_418 (BatchN (None, 4, 4, 30)     120         concatenate_376[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_409 (Activation)     (None, 4, 4, 30)     0           batch_normalization_418[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_410 (Conv2D)             (None, 4, 4, 5)      3755        activation_409[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_377 (Concatenate)   (None, 4, 4, 35)     0           concatenate_376[0][0]            \n",
      "                                                                 conv2d_410[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_419 (BatchN (None, 4, 4, 35)     140         concatenate_377[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_410 (Activation)     (None, 4, 4, 35)     0           batch_normalization_419[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_411 (Conv2D)             (None, 4, 4, 5)      4380        activation_410[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_378 (Concatenate)   (None, 4, 4, 40)     0           concatenate_377[0][0]            \n",
      "                                                                 conv2d_411[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_420 (BatchN (None, 4, 4, 40)     160         concatenate_378[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_411 (Activation)     (None, 4, 4, 40)     0           batch_normalization_420[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_412 (Conv2D)             (None, 4, 4, 5)      5005        activation_411[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_379 (Concatenate)   (None, 4, 4, 45)     0           concatenate_378[0][0]            \n",
      "                                                                 conv2d_412[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_421 (BatchN (None, 4, 4, 45)     180         concatenate_379[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_412 (Activation)     (None, 4, 4, 45)     0           batch_normalization_421[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_413 (Conv2D)             (None, 4, 4, 5)      5630        activation_412[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_380 (Concatenate)   (None, 4, 4, 50)     0           concatenate_379[0][0]            \n",
      "                                                                 conv2d_413[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_422 (BatchN (None, 4, 4, 50)     200         concatenate_380[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_413 (Activation)     (None, 4, 4, 50)     0           batch_normalization_422[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_414 (Conv2D)             (None, 4, 4, 5)      6255        activation_413[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_381 (Concatenate)   (None, 4, 4, 55)     0           concatenate_380[0][0]            \n",
      "                                                                 conv2d_414[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_423 (BatchN (None, 4, 4, 55)     220         concatenate_381[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_414 (Activation)     (None, 4, 4, 55)     0           batch_normalization_423[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_415 (Conv2D)             (None, 4, 4, 5)      6880        activation_414[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_382 (Concatenate)   (None, 4, 4, 60)     0           concatenate_381[0][0]            \n",
      "                                                                 conv2d_415[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_424 (BatchN (None, 4, 4, 60)     240         concatenate_382[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_415 (Activation)     (None, 4, 4, 60)     0           batch_normalization_424[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_416 (Conv2D)             (None, 4, 4, 5)      7505        activation_415[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_383 (Concatenate)   (None, 4, 4, 65)     0           concatenate_382[0][0]            \n",
      "                                                                 conv2d_416[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_425 (BatchN (None, 4, 4, 65)     260         concatenate_383[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_416 (Activation)     (None, 4, 4, 65)     0           batch_normalization_425[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_31 (AveragePo (None, 2, 2, 65)     0           activation_416[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 260)          0           average_pooling2d_31[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 10)           2610        flatten_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 747,231\n",
      "Trainable params: 741,257\n",
      "Non-trainable params: 5,974\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[input], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c0QWFrXej4RG"
   },
   "outputs": [],
   "source": [
    "# determine Loss function and Optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(), metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "CTy-dtN4j4Tc",
    "outputId": "65bc9fcb-5063-4473-f4bb-de28c6ef0f9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 1.5275 - acc: 0.4362Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 13s 1ms/sample - loss: 1.4382 - acc: 0.4974\n",
      "781/781 [==============================] - 186s 238ms/step - loss: 1.5274 - acc: 0.4364 - val_loss: 1.3837 - val_acc: 0.4974\n",
      "Epoch 2/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 1.2076 - acc: 0.5613Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 1.5742 - acc: 0.5652\n",
      "781/781 [==============================] - 183s 234ms/step - loss: 1.2077 - acc: 0.5612 - val_loss: 1.2507 - val_acc: 0.5652\n",
      "Epoch 3/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.9900 - acc: 0.6452Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 10s 1ms/sample - loss: 1.3140 - acc: 0.6044\n",
      "781/781 [==============================] - 183s 234ms/step - loss: 0.9901 - acc: 0.6451 - val_loss: 1.2087 - val_acc: 0.6044\n",
      "Epoch 4/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.8670 - acc: 0.6949Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.7690 - acc: 0.6924\n",
      "781/781 [==============================] - 183s 234ms/step - loss: 0.8669 - acc: 0.6950 - val_loss: 0.8870 - val_acc: 0.6924\n",
      "Epoch 5/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.7756 - acc: 0.7274Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.7836 - acc: 0.6953\n",
      "781/781 [==============================] - 183s 234ms/step - loss: 0.7755 - acc: 0.7274 - val_loss: 0.8746 - val_acc: 0.6953\n",
      "Epoch 6/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.7118 - acc: 0.7523Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.9152 - acc: 0.7054\n",
      "781/781 [==============================] - 183s 235ms/step - loss: 0.7118 - acc: 0.7524 - val_loss: 0.8560 - val_acc: 0.7054\n",
      "Epoch 7/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.6602 - acc: 0.7689Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.6077 - acc: 0.7401\n",
      "781/781 [==============================] - 185s 236ms/step - loss: 0.6604 - acc: 0.7689 - val_loss: 0.7374 - val_acc: 0.7401\n",
      "Epoch 8/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.6170 - acc: 0.7850Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.7451 - acc: 0.7286\n",
      "781/781 [==============================] - 185s 236ms/step - loss: 0.6167 - acc: 0.7851 - val_loss: 0.7916 - val_acc: 0.7286\n",
      "Epoch 9/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.5827 - acc: 0.7996Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.6747 - acc: 0.7739\n",
      "781/781 [==============================] - 185s 236ms/step - loss: 0.5825 - acc: 0.7997 - val_loss: 0.6578 - val_acc: 0.7739\n",
      "Epoch 10/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.5473 - acc: 0.8092Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.6126 - acc: 0.7666\n",
      "781/781 [==============================] - 183s 235ms/step - loss: 0.5474 - acc: 0.8092 - val_loss: 0.6991 - val_acc: 0.7666\n",
      "Epoch 11/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.5221 - acc: 0.8213Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.6866 - acc: 0.7694\n",
      "781/781 [==============================] - 183s 235ms/step - loss: 0.5226 - acc: 0.8211 - val_loss: 0.6816 - val_acc: 0.7694\n",
      "Epoch 12/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.5038 - acc: 0.8255Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.5435 - acc: 0.7885\n",
      "781/781 [==============================] - 183s 234ms/step - loss: 0.5039 - acc: 0.8255 - val_loss: 0.6486 - val_acc: 0.7885\n",
      "Epoch 13/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.4813 - acc: 0.8331Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.3991 - acc: 0.8047\n",
      "781/781 [==============================] - 184s 235ms/step - loss: 0.4811 - acc: 0.8332 - val_loss: 0.5849 - val_acc: 0.8047\n",
      "Epoch 14/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.4567 - acc: 0.8416Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.5540 - acc: 0.7849\n",
      "781/781 [==============================] - 183s 234ms/step - loss: 0.4566 - acc: 0.8417 - val_loss: 0.6536 - val_acc: 0.7849\n",
      "Epoch 15/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.4379 - acc: 0.8481Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.6660 - acc: 0.8011\n",
      "781/781 [==============================] - 184s 235ms/step - loss: 0.4378 - acc: 0.8481 - val_loss: 0.6330 - val_acc: 0.8011\n",
      "Epoch 16/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.4213 - acc: 0.8549Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.5207 - acc: 0.8034\n",
      "781/781 [==============================] - 182s 233ms/step - loss: 0.4212 - acc: 0.8550 - val_loss: 0.5885 - val_acc: 0.8034\n",
      "Epoch 17/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.4086 - acc: 0.8589Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.4926 - acc: 0.8185\n",
      "781/781 [==============================] - 182s 233ms/step - loss: 0.4087 - acc: 0.8589 - val_loss: 0.5474 - val_acc: 0.8185\n",
      "Epoch 18/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3931 - acc: 0.8635Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.7165 - acc: 0.7686\n",
      "781/781 [==============================] - 182s 233ms/step - loss: 0.3930 - acc: 0.8636 - val_loss: 0.7313 - val_acc: 0.7686\n",
      "Epoch 19/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3815 - acc: 0.8663Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.4423 - acc: 0.8287\n",
      "781/781 [==============================] - 181s 232ms/step - loss: 0.3814 - acc: 0.8663 - val_loss: 0.5105 - val_acc: 0.8287\n",
      "Epoch 20/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3705 - acc: 0.8704Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.5411 - acc: 0.8122\n",
      "781/781 [==============================] - 182s 233ms/step - loss: 0.3704 - acc: 0.8705 - val_loss: 0.5606 - val_acc: 0.8122\n",
      "Epoch 21/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3542 - acc: 0.8774Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.6598 - acc: 0.7990\n",
      "781/781 [==============================] - 182s 233ms/step - loss: 0.3543 - acc: 0.8774 - val_loss: 0.6226 - val_acc: 0.7990\n",
      "Epoch 22/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3448 - acc: 0.8801Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.7122 - acc: 0.8209\n",
      "781/781 [==============================] - 182s 233ms/step - loss: 0.3454 - acc: 0.8800 - val_loss: 0.5809 - val_acc: 0.8209\n",
      "Epoch 23/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3381 - acc: 0.8830Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.6153 - acc: 0.8135\n",
      "781/781 [==============================] - 183s 234ms/step - loss: 0.3380 - acc: 0.8830 - val_loss: 0.5634 - val_acc: 0.8135\n",
      "Epoch 24/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3289 - acc: 0.8860Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.5751 - acc: 0.8294\n",
      "781/781 [==============================] - 183s 234ms/step - loss: 0.3288 - acc: 0.8860 - val_loss: 0.5340 - val_acc: 0.8294\n",
      "Epoch 25/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3184 - acc: 0.8898Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.4361 - acc: 0.8350\n",
      "781/781 [==============================] - 184s 235ms/step - loss: 0.3182 - acc: 0.8899 - val_loss: 0.5087 - val_acc: 0.8350\n",
      "Epoch 26/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3128 - acc: 0.8905Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.5287 - acc: 0.8252\n",
      "781/781 [==============================] - 184s 235ms/step - loss: 0.3129 - acc: 0.8905 - val_loss: 0.5501 - val_acc: 0.8252\n",
      "Epoch 27/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3015 - acc: 0.8948Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.6227 - acc: 0.8130\n",
      "781/781 [==============================] - 183s 235ms/step - loss: 0.3013 - acc: 0.8949 - val_loss: 0.5863 - val_acc: 0.8130\n",
      "Epoch 28/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2948 - acc: 0.8964Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.7176 - acc: 0.8146\n",
      "781/781 [==============================] - 183s 235ms/step - loss: 0.2948 - acc: 0.8964 - val_loss: 0.5912 - val_acc: 0.8146\n",
      "Epoch 29/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2887 - acc: 0.8999Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.6390 - acc: 0.8458\n",
      "781/781 [==============================] - 184s 236ms/step - loss: 0.2886 - acc: 0.8999 - val_loss: 0.4839 - val_acc: 0.8458\n",
      "Epoch 30/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2746 - acc: 0.9037Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.6728 - acc: 0.8389\n",
      "781/781 [==============================] - 183s 234ms/step - loss: 0.2749 - acc: 0.9037 - val_loss: 0.5129 - val_acc: 0.8389\n",
      "Epoch 31/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2753 - acc: 0.9033Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.8001 - acc: 0.8289\n",
      "781/781 [==============================] - 185s 237ms/step - loss: 0.2755 - acc: 0.9033 - val_loss: 0.5541 - val_acc: 0.8289\n",
      "Epoch 32/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2650 - acc: 0.9076Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.7446 - acc: 0.8447\n",
      "781/781 [==============================] - 185s 237ms/step - loss: 0.2650 - acc: 0.9076 - val_loss: 0.5066 - val_acc: 0.8447\n",
      "Epoch 33/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2663 - acc: 0.9062Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 1.0106 - acc: 0.8115\n",
      "781/781 [==============================] - 185s 236ms/step - loss: 0.2663 - acc: 0.9062 - val_loss: 0.6585 - val_acc: 0.8115\n",
      "Epoch 34/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2530 - acc: 0.9133Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.4725 - acc: 0.8524\n",
      "781/781 [==============================] - 184s 236ms/step - loss: 0.2533 - acc: 0.9132 - val_loss: 0.4882 - val_acc: 0.8524\n",
      "Epoch 35/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2439 - acc: 0.9147Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.5355 - acc: 0.8450\n",
      "781/781 [==============================] - 185s 237ms/step - loss: 0.2439 - acc: 0.9147 - val_loss: 0.5012 - val_acc: 0.8450\n",
      "Epoch 36/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2429 - acc: 0.9157Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.4714 - acc: 0.8458\n",
      "781/781 [==============================] - 184s 236ms/step - loss: 0.2429 - acc: 0.9158 - val_loss: 0.5050 - val_acc: 0.8458\n",
      "Epoch 37/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2397 - acc: 0.9160Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.5399 - acc: 0.8448\n",
      "781/781 [==============================] - 184s 236ms/step - loss: 0.2397 - acc: 0.9161 - val_loss: 0.4885 - val_acc: 0.8448\n",
      "Epoch 38/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2331 - acc: 0.9185Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.8010 - acc: 0.8393\n",
      "781/781 [==============================] - 184s 236ms/step - loss: 0.2330 - acc: 0.9186 - val_loss: 0.5291 - val_acc: 0.8393\n",
      "Epoch 39/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2278 - acc: 0.9198Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.5445 - acc: 0.8517\n",
      "781/781 [==============================] - 185s 236ms/step - loss: 0.2278 - acc: 0.9198 - val_loss: 0.4968 - val_acc: 0.8517\n",
      "Epoch 40/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2221 - acc: 0.9221Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.6301 - acc: 0.8407\n",
      "781/781 [==============================] - 185s 237ms/step - loss: 0.2221 - acc: 0.9221 - val_loss: 0.5120 - val_acc: 0.8407\n",
      "Epoch 41/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2149 - acc: 0.9229Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.6066 - acc: 0.8432\n",
      "781/781 [==============================] - 185s 237ms/step - loss: 0.2150 - acc: 0.9229 - val_loss: 0.5300 - val_acc: 0.8432\n",
      "Epoch 42/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2155 - acc: 0.9237Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.4853 - acc: 0.8459\n",
      "781/781 [==============================] - 185s 237ms/step - loss: 0.2154 - acc: 0.9238 - val_loss: 0.4969 - val_acc: 0.8459\n",
      "Epoch 43/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2074 - acc: 0.9271Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.7062 - acc: 0.8567\n",
      "781/781 [==============================] - 185s 237ms/step - loss: 0.2077 - acc: 0.9270 - val_loss: 0.4684 - val_acc: 0.8567\n",
      "Epoch 44/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2027 - acc: 0.9279Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.7565 - acc: 0.8354\n",
      "781/781 [==============================] - 185s 237ms/step - loss: 0.2030 - acc: 0.9279 - val_loss: 0.5808 - val_acc: 0.8354\n",
      "Epoch 45/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2004 - acc: 0.9297Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.6132 - acc: 0.8532\n",
      "781/781 [==============================] - 185s 237ms/step - loss: 0.2004 - acc: 0.9298 - val_loss: 0.5076 - val_acc: 0.8532\n",
      "Epoch 46/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1944 - acc: 0.9306Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.5246 - acc: 0.8232\n",
      "781/781 [==============================] - 185s 237ms/step - loss: 0.1945 - acc: 0.9305 - val_loss: 0.6643 - val_acc: 0.8232\n",
      "Epoch 47/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1928 - acc: 0.9318Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.5511 - acc: 0.8419\n",
      "781/781 [==============================] - 185s 237ms/step - loss: 0.1927 - acc: 0.9318 - val_loss: 0.5415 - val_acc: 0.8419\n",
      "Epoch 48/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1881 - acc: 0.9341Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.6489 - acc: 0.8280\n",
      "781/781 [==============================] - 185s 237ms/step - loss: 0.1880 - acc: 0.9341 - val_loss: 0.6352 - val_acc: 0.8280\n",
      "Epoch 49/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1840 - acc: 0.9345Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.4874 - acc: 0.8336\n",
      "781/781 [==============================] - 185s 237ms/step - loss: 0.1840 - acc: 0.9345 - val_loss: 0.5864 - val_acc: 0.8336\n",
      "Epoch 50/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1864 - acc: 0.9348Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.5959 - acc: 0.8535\n",
      "781/781 [==============================] - 184s 236ms/step - loss: 0.1864 - acc: 0.9348 - val_loss: 0.4996 - val_acc: 0.8535\n",
      "Epoch 51/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1799 - acc: 0.9363Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.5982 - acc: 0.8551\n",
      "781/781 [==============================] - 185s 237ms/step - loss: 0.1797 - acc: 0.9363 - val_loss: 0.5011 - val_acc: 0.8551\n",
      "Epoch 52/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1766 - acc: 0.9357Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.7926 - acc: 0.8349\n",
      "781/781 [==============================] - 185s 237ms/step - loss: 0.1766 - acc: 0.9357 - val_loss: 0.6149 - val_acc: 0.8349\n",
      "Epoch 53/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1759 - acc: 0.9372Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.5796 - acc: 0.8486\n",
      "781/781 [==============================] - 184s 235ms/step - loss: 0.1760 - acc: 0.9371 - val_loss: 0.5058 - val_acc: 0.8486\n",
      "Epoch 54/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1700 - acc: 0.9401Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.6026 - acc: 0.8495\n",
      "781/781 [==============================] - 184s 235ms/step - loss: 0.1702 - acc: 0.9400 - val_loss: 0.5278 - val_acc: 0.8495\n",
      "Epoch 55/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1680 - acc: 0.9395Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.7365 - acc: 0.8562\n",
      "781/781 [==============================] - 184s 235ms/step - loss: 0.1680 - acc: 0.9395 - val_loss: 0.5138 - val_acc: 0.8562\n",
      "Epoch 56/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1618 - acc: 0.9425Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.5387 - acc: 0.8668\n",
      "781/781 [==============================] - 184s 236ms/step - loss: 0.1618 - acc: 0.9424 - val_loss: 0.4570 - val_acc: 0.8668\n",
      "Epoch 57/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1607 - acc: 0.9420Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.3619 - acc: 0.8581\n",
      "781/781 [==============================] - 184s 236ms/step - loss: 0.1606 - acc: 0.9420 - val_loss: 0.5153 - val_acc: 0.8581\n",
      "Epoch 58/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1586 - acc: 0.9444Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.5780 - acc: 0.8325\n",
      "781/781 [==============================] - 184s 236ms/step - loss: 0.1586 - acc: 0.9443 - val_loss: 0.6298 - val_acc: 0.8325\n",
      "Epoch 59/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1538 - acc: 0.9449Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.7664 - acc: 0.8466\n",
      "781/781 [==============================] - 185s 237ms/step - loss: 0.1537 - acc: 0.9449 - val_loss: 0.5534 - val_acc: 0.8466\n",
      "Epoch 60/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1519 - acc: 0.9464Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.5921 - acc: 0.8507\n",
      "781/781 [==============================] - 185s 236ms/step - loss: 0.1520 - acc: 0.9463 - val_loss: 0.5623 - val_acc: 0.8507\n",
      "Epoch 61/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1540 - acc: 0.9462Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.3668 - acc: 0.8671\n",
      "781/781 [==============================] - 183s 235ms/step - loss: 0.1540 - acc: 0.9462 - val_loss: 0.4643 - val_acc: 0.8671\n",
      "Epoch 62/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1472 - acc: 0.9482Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.5684 - acc: 0.8559\n",
      "781/781 [==============================] - 184s 236ms/step - loss: 0.1473 - acc: 0.9481 - val_loss: 0.5186 - val_acc: 0.8559\n",
      "Epoch 63/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1476 - acc: 0.9478Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.5775 - acc: 0.8475\n",
      "781/781 [==============================] - 184s 236ms/step - loss: 0.1476 - acc: 0.9479 - val_loss: 0.5531 - val_acc: 0.8475\n",
      "Epoch 64/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1432 - acc: 0.9494Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.6088 - acc: 0.8444\n",
      "781/781 [==============================] - 184s 235ms/step - loss: 0.1432 - acc: 0.9494 - val_loss: 0.6028 - val_acc: 0.8444\n",
      "Epoch 65/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1415 - acc: 0.9488Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.5566 - acc: 0.8535\n",
      "781/781 [==============================] - 184s 236ms/step - loss: 0.1415 - acc: 0.9489 - val_loss: 0.5429 - val_acc: 0.8535\n",
      "Epoch 66/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1400 - acc: 0.9506Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.6963 - acc: 0.8622\n",
      "781/781 [==============================] - 184s 236ms/step - loss: 0.1400 - acc: 0.9506 - val_loss: 0.5102 - val_acc: 0.8622\n",
      "Epoch 67/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1372 - acc: 0.9521Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.8653 - acc: 0.8293\n",
      "781/781 [==============================] - 185s 236ms/step - loss: 0.1372 - acc: 0.9521 - val_loss: 0.6689 - val_acc: 0.8293\n",
      "Epoch 68/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1324 - acc: 0.9527Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.6295 - acc: 0.8695\n",
      "781/781 [==============================] - 184s 235ms/step - loss: 0.1325 - acc: 0.9527 - val_loss: 0.5008 - val_acc: 0.8695\n",
      "Epoch 69/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1354 - acc: 0.9516Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.6534 - acc: 0.8510\n",
      "781/781 [==============================] - 183s 235ms/step - loss: 0.1353 - acc: 0.9516 - val_loss: 0.5953 - val_acc: 0.8510\n",
      "Epoch 70/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1341 - acc: 0.9517Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.5366 - acc: 0.8482\n",
      "781/781 [==============================] - 185s 237ms/step - loss: 0.1341 - acc: 0.9517 - val_loss: 0.5657 - val_acc: 0.8482\n",
      "Epoch 71/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1288 - acc: 0.9551Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.4635 - acc: 0.8641\n",
      "781/781 [==============================] - 184s 236ms/step - loss: 0.1287 - acc: 0.9552 - val_loss: 0.5063 - val_acc: 0.8641\n",
      "Epoch 72/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1252 - acc: 0.9558Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.4408 - acc: 0.8674\n",
      "781/781 [==============================] - 184s 236ms/step - loss: 0.1252 - acc: 0.9558 - val_loss: 0.5141 - val_acc: 0.8674\n",
      "Epoch 73/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1214 - acc: 0.9566Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.7239 - acc: 0.8531\n",
      "781/781 [==============================] - 185s 237ms/step - loss: 0.1214 - acc: 0.9566 - val_loss: 0.5747 - val_acc: 0.8531\n",
      "Epoch 74/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.9553Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.5088 - acc: 0.8679\n",
      "781/781 [==============================] - 185s 237ms/step - loss: 0.1252 - acc: 0.9553 - val_loss: 0.4984 - val_acc: 0.8679\n",
      "Epoch 75/75\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1203 - acc: 0.9561Epoch 1/75\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 11s 1ms/sample - loss: 0.8336 - acc: 0.8439\n",
      "781/781 [==============================] - 185s 237ms/step - loss: 0.1203 - acc: 0.9561 - val_loss: 0.6271 - val_acc: 0.8439\n",
      "> 84.390\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMEAAAEICAYAAADm0pBUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/\n",
    "def summarize_diagnostics(history):\n",
    "\t# plot loss\n",
    "\tpyplot.subplot(121)\n",
    "\tpyplot.title('Cross Entropy Loss')\n",
    "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "\tpyplot.show()\n",
    "\t\n",
    " \n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t\n",
    "\t# define model\n",
    "\t\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "\t# prepare iterator\n",
    "\tit_train = datagen.flow(X_train, y_train, batch_size=64)\n",
    "\t# fit model\n",
    "\tsteps = int(X_train.shape[0] / 64)\n",
    "\thistory = model.fit_generator(it_train, steps_per_epoch=steps, epochs=75, validation_data=(X_test, y_test), verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# learning curves\n",
    "\tsummarize_diagnostics(history)\n",
    " \n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_loss: 0.1203   train_acc: 0.9561 \n",
    "\n",
    "val_loss: 0.6271   val_acc: 0.8439"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model without dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "59osdY7zj4Vr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\santosh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "# Dense Block\n",
    "def denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    temp = input\n",
    "    for _ in range(l): \n",
    "        BatchNorm = layers.BatchNormalization()(temp)\n",
    "        relu = layers.Activation('relu')(BatchNorm)\n",
    "        Conv2D_3_3 = layers.Conv2D(int(num_filter*compression), (5,5), use_bias=False ,padding='same')(relu)\n",
    "        if dropout_rate>0:\n",
    "            Conv2D_3_3 = layers.Dropout(dropout_rate)(Conv2D_3_3)\n",
    "        concat = layers.Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
    "        \n",
    "        temp = concat\n",
    "        \n",
    "    return temp\n",
    "\n",
    "## transition Blosck\n",
    "def transition(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    Conv2D_BottleNeck = layers.Conv2D(int(num_filter*compression), (5,5), use_bias=False ,padding='same')(relu)\n",
    "    if dropout_rate>0:\n",
    "         Conv2D_BottleNeck = layers.Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
    "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
    "    return avg\n",
    "\n",
    "#output layer\n",
    "def output_layer(input):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    AvgPooling = layers. MaxPooling2D(pool_size=(2,2))(relu)\n",
    "    \n",
    "    output = layers.Conv2D(filters=10,kernel_size=(2,2),activation='softmax')(AvgPooling)\n",
    "   \n",
    "    flat = layers.Flatten()(output)    \n",
    "    return flat\n",
    "\n",
    "\n",
    "\n",
    "num_filter = 12\n",
    "dropout_rate = 0\n",
    "l = 12\n",
    "input = layers.Input(shape=(img_height, img_width, channel,))\n",
    "First_Conv2D = layers.Conv2D(32, (3,3), use_bias=False ,padding='same')(input)\n",
    "\n",
    "First_Block = denseblock(First_Conv2D,10, dropout_rate)\n",
    "First_Transition = transition(First_Block, 64, dropout_rate)\n",
    "\n",
    "Second_Block = denseblock(First_Transition, 10, dropout_rate)\n",
    "Second_Transition = transition(Second_Block, 32, dropout_rate)\n",
    "\n",
    "Third_Block = denseblock(Second_Transition, num_filter, dropout_rate)\n",
    "Third_Transition = transition(Third_Block, 32, dropout_rate)\n",
    "\n",
    "Last_Block = denseblock(Third_Transition,  num_filter, dropout_rate)\n",
    "output = output_layer(Last_Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "P9Ww0AmJj4Zj",
    "outputId": "2ac69c44-3184-4fdf-f007-541ac4d3ce4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 32, 32)   864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 32, 32, 32)   128         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 32, 32, 32)   0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 5)    4000        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 32, 32, 37)   0           conv2d[0][0]                     \n",
      "                                                                 conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 32, 32, 37)   148         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 37)   0           batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 5)    4625        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 42)   0           concatenate[0][0]                \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 32, 32, 42)   168         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 42)   0           batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 5)    5250        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 47)   0           concatenate_1[0][0]              \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 32, 32, 47)   188         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 47)   0           batch_normalization_v1_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 5)    5875        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 32, 32, 52)   0           concatenate_2[0][0]              \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 32, 32, 52)   208         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 52)   0           batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 5)    6500        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 32, 32, 57)   0           concatenate_3[0][0]              \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 32, 32, 57)   228         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 57)   0           batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 5)    7125        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 32, 32, 62)   0           concatenate_4[0][0]              \n",
      "                                                                 conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 32, 32, 62)   248         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 62)   0           batch_normalization_v1_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 5)    7750        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 32, 32, 67)   0           concatenate_5[0][0]              \n",
      "                                                                 conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_7 (Batch (None, 32, 32, 67)   268         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 67)   0           batch_normalization_v1_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 5)    8375        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 32, 32, 72)   0           concatenate_6[0][0]              \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_8 (Batch (None, 32, 32, 72)   288         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 72)   0           batch_normalization_v1_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 5)    9000        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 32, 32, 77)   0           concatenate_7[0][0]              \n",
      "                                                                 conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_9 (Batch (None, 32, 32, 77)   308         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 77)   0           batch_normalization_v1_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 5)    9625        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 32, 32, 82)   0           concatenate_8[0][0]              \n",
      "                                                                 conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_10 (Batc (None, 32, 32, 82)   328         concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 82)   0           batch_normalization_v1_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 5)    10250       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 32, 32, 87)   0           concatenate_9[0][0]              \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_11 (Batc (None, 32, 32, 87)   348         concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 87)   0           batch_normalization_v1_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 5)    10875       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 32, 32, 92)   0           concatenate_10[0][0]             \n",
      "                                                                 conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_12 (Batc (None, 32, 32, 92)   368         concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 92)   0           batch_normalization_v1_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 32)   73600       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 16, 16, 32)   0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_13 (Batc (None, 16, 16, 32)   128         average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 32)   0           batch_normalization_v1_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 5)    4000        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 16, 16, 37)   0           average_pooling2d[0][0]          \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_14 (Batc (None, 16, 16, 37)   148         concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16, 16, 37)   0           batch_normalization_v1_14[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 16, 5)    4625        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 16, 16, 42)   0           concatenate_12[0][0]             \n",
      "                                                                 conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_15 (Batc (None, 16, 16, 42)   168         concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16, 16, 42)   0           batch_normalization_v1_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 16, 16, 5)    5250        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 16, 16, 47)   0           concatenate_13[0][0]             \n",
      "                                                                 conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_16 (Batc (None, 16, 16, 47)   188         concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 47)   0           batch_normalization_v1_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 16, 16, 5)    5875        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 16, 16, 52)   0           concatenate_14[0][0]             \n",
      "                                                                 conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_17 (Batc (None, 16, 16, 52)   208         concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 16, 52)   0           batch_normalization_v1_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 16, 16, 5)    6500        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 16, 16, 57)   0           concatenate_15[0][0]             \n",
      "                                                                 conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_18 (Batc (None, 16, 16, 57)   228         concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16, 16, 57)   0           batch_normalization_v1_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 16, 16, 5)    7125        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 16, 16, 62)   0           concatenate_16[0][0]             \n",
      "                                                                 conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_19 (Batc (None, 16, 16, 62)   248         concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 16, 16, 62)   0           batch_normalization_v1_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 16, 16, 5)    7750        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 16, 16, 67)   0           concatenate_17[0][0]             \n",
      "                                                                 conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_20 (Batc (None, 16, 16, 67)   268         concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 16, 16, 67)   0           batch_normalization_v1_20[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 16, 16, 5)    8375        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 16, 16, 72)   0           concatenate_18[0][0]             \n",
      "                                                                 conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_21 (Batc (None, 16, 16, 72)   288         concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 16, 16, 72)   0           batch_normalization_v1_21[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 16, 16, 5)    9000        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 16, 16, 77)   0           concatenate_19[0][0]             \n",
      "                                                                 conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_22 (Batc (None, 16, 16, 77)   308         concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 16, 16, 77)   0           batch_normalization_v1_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 16, 16, 5)    9625        activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 16, 16, 82)   0           concatenate_20[0][0]             \n",
      "                                                                 conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_23 (Batc (None, 16, 16, 82)   328         concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 16, 16, 82)   0           batch_normalization_v1_23[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 16, 16, 5)    10250       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 16, 16, 87)   0           concatenate_21[0][0]             \n",
      "                                                                 conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_24 (Batc (None, 16, 16, 87)   348         concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 16, 16, 87)   0           batch_normalization_v1_24[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 16, 16, 5)    10875       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 16, 16, 92)   0           concatenate_22[0][0]             \n",
      "                                                                 conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_25 (Batc (None, 16, 16, 92)   368         concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16, 16, 92)   0           batch_normalization_v1_25[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 16, 16, 16)   36800       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 8, 8, 16)     0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_26 (Batc (None, 8, 8, 16)     64          average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 8, 8, 16)     0           batch_normalization_v1_26[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 8, 8, 6)      2400        activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 8, 8, 22)     0           average_pooling2d_1[0][0]        \n",
      "                                                                 conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_27 (Batc (None, 8, 8, 22)     88          concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 8, 8, 22)     0           batch_normalization_v1_27[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 8, 8, 6)      3300        activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 8, 8, 28)     0           concatenate_24[0][0]             \n",
      "                                                                 conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_28 (Batc (None, 8, 8, 28)     112         concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 8, 8, 28)     0           batch_normalization_v1_28[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 8, 8, 6)      4200        activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 8, 8, 34)     0           concatenate_25[0][0]             \n",
      "                                                                 conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_29 (Batc (None, 8, 8, 34)     136         concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 8, 8, 34)     0           batch_normalization_v1_29[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 8, 8, 6)      5100        activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 8, 8, 40)     0           concatenate_26[0][0]             \n",
      "                                                                 conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_30 (Batc (None, 8, 8, 40)     160         concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 8, 8, 40)     0           batch_normalization_v1_30[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 8, 8, 6)      6000        activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)    (None, 8, 8, 46)     0           concatenate_27[0][0]             \n",
      "                                                                 conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_31 (Batc (None, 8, 8, 46)     184         concatenate_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 8, 8, 46)     0           batch_normalization_v1_31[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 8, 8, 6)      6900        activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, 8, 8, 52)     0           concatenate_28[0][0]             \n",
      "                                                                 conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_32 (Batc (None, 8, 8, 52)     208         concatenate_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 8, 8, 52)     0           batch_normalization_v1_32[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 8, 8, 6)      7800        activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)    (None, 8, 8, 58)     0           concatenate_29[0][0]             \n",
      "                                                                 conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 8, 8, 58)     232         concatenate_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 8, 8, 58)     0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 8, 8, 6)      8700        activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_31 (Concatenate)    (None, 8, 8, 64)     0           concatenate_30[0][0]             \n",
      "                                                                 conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 8, 8, 64)     256         concatenate_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 8, 8, 64)     0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 8, 8, 6)      9600        activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_32 (Concatenate)    (None, 8, 8, 70)     0           concatenate_31[0][0]             \n",
      "                                                                 conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 8, 8, 70)     280         concatenate_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 8, 8, 70)     0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 8, 8, 6)      10500       activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_33 (Concatenate)    (None, 8, 8, 76)     0           concatenate_32[0][0]             \n",
      "                                                                 conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 8, 8, 76)     304         concatenate_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 8, 8, 76)     0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 8, 8, 6)      11400       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_34 (Concatenate)    (None, 8, 8, 82)     0           concatenate_33[0][0]             \n",
      "                                                                 conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 8, 8, 82)     328         concatenate_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 8, 8, 82)     0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 8, 8, 6)      12300       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_35 (Concatenate)    (None, 8, 8, 88)     0           concatenate_34[0][0]             \n",
      "                                                                 conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 8, 8, 88)     352         concatenate_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 8, 8, 88)     0           batch_normalization_v1_38[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 8, 8, 16)     35200       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 4, 4, 16)     0           conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 4, 4, 16)     64          average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 4, 4, 16)     0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 4, 4, 6)      2400        activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_36 (Concatenate)    (None, 4, 4, 22)     0           average_pooling2d_2[0][0]        \n",
      "                                                                 conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 4, 4, 22)     88          concatenate_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 4, 4, 22)     0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 4, 4, 6)      3300        activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_37 (Concatenate)    (None, 4, 4, 28)     0           concatenate_36[0][0]             \n",
      "                                                                 conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 4, 4, 28)     112         concatenate_37[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 4, 4, 28)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 4, 4, 6)      4200        activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_38 (Concatenate)    (None, 4, 4, 34)     0           concatenate_37[0][0]             \n",
      "                                                                 conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 4, 4, 34)     136         concatenate_38[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 4, 4, 34)     0           batch_normalization_v1_42[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 4, 4, 6)      5100        activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_39 (Concatenate)    (None, 4, 4, 40)     0           concatenate_38[0][0]             \n",
      "                                                                 conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 4, 4, 40)     160         concatenate_39[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 4, 4, 40)     0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 4, 4, 6)      6000        activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_40 (Concatenate)    (None, 4, 4, 46)     0           concatenate_39[0][0]             \n",
      "                                                                 conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 4, 4, 46)     184         concatenate_40[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 4, 4, 46)     0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 4, 4, 6)      6900        activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_41 (Concatenate)    (None, 4, 4, 52)     0           concatenate_40[0][0]             \n",
      "                                                                 conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 4, 4, 52)     208         concatenate_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 4, 4, 52)     0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 4, 4, 6)      7800        activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_42 (Concatenate)    (None, 4, 4, 58)     0           concatenate_41[0][0]             \n",
      "                                                                 conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 4, 4, 58)     232         concatenate_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 4, 4, 58)     0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 4, 4, 6)      8700        activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_43 (Concatenate)    (None, 4, 4, 64)     0           concatenate_42[0][0]             \n",
      "                                                                 conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 4, 4, 64)     256         concatenate_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 4, 4, 64)     0           batch_normalization_v1_47[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 4, 4, 6)      9600        activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_44 (Concatenate)    (None, 4, 4, 70)     0           concatenate_43[0][0]             \n",
      "                                                                 conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 4, 4, 70)     280         concatenate_44[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 4, 4, 70)     0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 4, 4, 6)      10500       activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_45 (Concatenate)    (None, 4, 4, 76)     0           concatenate_44[0][0]             \n",
      "                                                                 conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 4, 4, 76)     304         concatenate_45[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 4, 4, 76)     0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 4, 4, 6)      11400       activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_46 (Concatenate)    (None, 4, 4, 82)     0           concatenate_45[0][0]             \n",
      "                                                                 conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 4, 4, 82)     328         concatenate_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 4, 4, 82)     0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 4, 4, 6)      12300       activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_47 (Concatenate)    (None, 4, 4, 88)     0           concatenate_46[0][0]             \n",
      "                                                                 conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 4, 4, 88)     352         concatenate_47[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 4, 4, 88)     0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 2, 2, 88)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 1, 1, 10)     3530        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 10)           0           conv2d_52[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 516,750\n",
      "Trainable params: 510,822\n",
      "Non-trainable params: 5,928\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[input], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1WQT_osgghe5"
   },
   "outputs": [],
   "source": [
    "# determine Loss function and Optimizer\n",
    "#a=keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "YIZBdVo-gwLV",
    "outputId": "c3f7a8be-342b-401d-e50e-1ee8745dda13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "10000/10000 [==============================] - 32s 3ms/sample - loss: 1.1354 - acc: 0.58700s - loss: 1.1351 - acc: 0.587\n",
      "834/834 [==============================] - 473s 567ms/step - loss: 1.5155 - acc: 0.4424 - val_loss: 1.1356 - val_acc: 0.5870\n",
      "Epoch 2/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 1.2174 - acc: 0.6069\n",
      "834/834 [==============================] - 472s 566ms/step - loss: 1.0230 - acc: 0.6337 - val_loss: 1.2175 - val_acc: 0.6069\n",
      "Epoch 3/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 1.0541 - acc: 0.6593\n",
      "834/834 [==============================] - 480s 575ms/step - loss: 0.8244 - acc: 0.7094 - val_loss: 1.0544 - val_acc: 0.6593\n",
      "Epoch 4/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.8890 - acc: 0.7024\n",
      "834/834 [==============================] - 492s 590ms/step - loss: 0.7119 - acc: 0.7505 - val_loss: 0.8894 - val_acc: 0.7024\n",
      "Epoch 5/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.7166 - acc: 0.7549\n",
      "834/834 [==============================] - 480s 575ms/step - loss: 0.6386 - acc: 0.7766 - val_loss: 0.7168 - val_acc: 0.7549\n",
      "Epoch 6/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.7537 - acc: 0.7492\n",
      "834/834 [==============================] - 480s 576ms/step - loss: 0.5874 - acc: 0.7945 - val_loss: 0.7538 - val_acc: 0.7492\n",
      "Epoch 7/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.6768 - acc: 0.7707\n",
      "834/834 [==============================] - 479s 574ms/step - loss: 0.5394 - acc: 0.8137 - val_loss: 0.6772 - val_acc: 0.7707\n",
      "Epoch 8/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.7362 - acc: 0.7661\n",
      "834/834 [==============================] - 481s 577ms/step - loss: 0.5073 - acc: 0.8259 - val_loss: 0.7368 - val_acc: 0.7661\n",
      "Epoch 9/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.6417 - acc: 0.7787\n",
      "834/834 [==============================] - 479s 574ms/step - loss: 0.4746 - acc: 0.8372 - val_loss: 0.6421 - val_acc: 0.7787\n",
      "Epoch 10/200\n",
      "10000/10000 [==============================] - 32s 3ms/sample - loss: 0.5711 - acc: 0.8113\n",
      "834/834 [==============================] - 483s 580ms/step - loss: 0.4458 - acc: 0.8461 - val_loss: 0.5715 - val_acc: 0.8113\n",
      "Epoch 11/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.6493 - acc: 0.7828\n",
      "834/834 [==============================] - 481s 576ms/step - loss: 0.4247 - acc: 0.8529 - val_loss: 0.6496 - val_acc: 0.7828\n",
      "Epoch 12/200\n",
      "10000/10000 [==============================] - 32s 3ms/sample - loss: 0.5485 - acc: 0.8191\n",
      "834/834 [==============================] - 483s 579ms/step - loss: 0.4053 - acc: 0.8616 - val_loss: 0.5488 - val_acc: 0.8191\n",
      "Epoch 13/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.4963 - acc: 0.8348\n",
      "834/834 [==============================] - 479s 575ms/step - loss: 0.3890 - acc: 0.8665 - val_loss: 0.4969 - val_acc: 0.8348\n",
      "Epoch 14/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.7014 - acc: 0.7853\n",
      "834/834 [==============================] - 480s 576ms/step - loss: 0.3682 - acc: 0.8727 - val_loss: 0.7019 - val_acc: 0.7853\n",
      "Epoch 15/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.5631 - acc: 0.8072\n",
      "834/834 [==============================] - 478s 573ms/step - loss: 0.3538 - acc: 0.8779 - val_loss: 0.5632 - val_acc: 0.8072\n",
      "Epoch 16/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.5635 - acc: 0.8176\n",
      "834/834 [==============================] - 483s 579ms/step - loss: 0.3380 - acc: 0.8833 - val_loss: 0.5640 - val_acc: 0.8176\n",
      "Epoch 17/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.6366 - acc: 0.8033\n",
      "834/834 [==============================] - 482s 578ms/step - loss: 0.3254 - acc: 0.8880 - val_loss: 0.6372 - val_acc: 0.8033\n",
      "Epoch 18/200\n",
      "10000/10000 [==============================] - 32s 3ms/sample - loss: 0.4180 - acc: 0.86253s - loss: \n",
      "834/834 [==============================] - 478s 573ms/step - loss: 0.3123 - acc: 0.8925 - val_loss: 0.4185 - val_acc: 0.8625\n",
      "Epoch 19/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.5023 - acc: 0.83674s - \n",
      "834/834 [==============================] - 480s 576ms/step - loss: 0.3030 - acc: 0.8945 - val_loss: 0.5026 - val_acc: 0.8367\n",
      "Epoch 20/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.7203 - acc: 0.7853\n",
      "834/834 [==============================] - 482s 578ms/step - loss: 0.2940 - acc: 0.8978 - val_loss: 0.7206 - val_acc: 0.7853\n",
      "Epoch 21/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.4552 - acc: 0.8546\n",
      "834/834 [==============================] - 482s 578ms/step - loss: 0.2850 - acc: 0.9012 - val_loss: 0.4555 - val_acc: 0.8546\n",
      "Epoch 22/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5610 - acc: 0.8186\n",
      "834/834 [==============================] - 477s 572ms/step - loss: 0.2762 - acc: 0.9055 - val_loss: 0.5611 - val_acc: 0.8186\n",
      "Epoch 23/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.5169 - acc: 0.8409\n",
      "834/834 [==============================] - 488s 585ms/step - loss: 0.2656 - acc: 0.9081 - val_loss: 0.5173 - val_acc: 0.8409\n",
      "Epoch 24/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.4558 - acc: 0.8516\n",
      "834/834 [==============================] - 484s 580ms/step - loss: 0.2579 - acc: 0.9100 - val_loss: 0.4568 - val_acc: 0.8516\n",
      "Epoch 25/200\n",
      "10000/10000 [==============================] - 32s 3ms/sample - loss: 0.4826 - acc: 0.8514\n",
      "834/834 [==============================] - 482s 577ms/step - loss: 0.2497 - acc: 0.9137 - val_loss: 0.4828 - val_acc: 0.8514\n",
      "Epoch 26/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.5597 - acc: 0.8248\n",
      "834/834 [==============================] - 478s 573ms/step - loss: 0.2417 - acc: 0.9149 - val_loss: 0.5601 - val_acc: 0.8248\n",
      "Epoch 27/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.4846 - acc: 0.8521\n",
      "834/834 [==============================] - 485s 581ms/step - loss: 0.2361 - acc: 0.9173 - val_loss: 0.4855 - val_acc: 0.8521\n",
      "Epoch 28/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.4264 - acc: 0.8663\n",
      "834/834 [==============================] - 487s 584ms/step - loss: 0.2232 - acc: 0.9216 - val_loss: 0.4269 - val_acc: 0.8663\n",
      "Epoch 29/200\n",
      "10000/10000 [==============================] - 32s 3ms/sample - loss: 0.4487 - acc: 0.8577\n",
      "834/834 [==============================] - 491s 588ms/step - loss: 0.2267 - acc: 0.9209 - val_loss: 0.4489 - val_acc: 0.8577\n",
      "Epoch 30/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.4225 - acc: 0.8692\n",
      "834/834 [==============================] - 490s 588ms/step - loss: 0.2179 - acc: 0.9224 - val_loss: 0.4232 - val_acc: 0.8692\n",
      "Epoch 31/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.5095 - acc: 0.84682s - loss: 0.5115 - - ETA: 0s - loss: 0.5079 - acc: 0.847 - ETA: 0s - loss: 0.5089 - acc: 0.8\n",
      "834/834 [==============================] - 488s 585ms/step - loss: 0.2112 - acc: 0.9266 - val_loss: 0.5098 - val_acc: 0.8468\n",
      "Epoch 32/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.4418 - acc: 0.8665\n",
      "834/834 [==============================] - 485s 582ms/step - loss: 0.2046 - acc: 0.9284 - val_loss: 0.4420 - val_acc: 0.8665\n",
      "Epoch 33/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.5283 - acc: 0.8510\n",
      "834/834 [==============================] - 485s 581ms/step - loss: 0.2032 - acc: 0.9291 - val_loss: 0.5291 - val_acc: 0.8510\n",
      "Epoch 34/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.4338 - acc: 0.8665\n",
      "834/834 [==============================] - 483s 579ms/step - loss: 0.1922 - acc: 0.9327 - val_loss: 0.4341 - val_acc: 0.8665\n",
      "Epoch 35/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.5994 - acc: 0.8407\n",
      "834/834 [==============================] - 482s 578ms/step - loss: 0.1911 - acc: 0.9327 - val_loss: 0.6002 - val_acc: 0.8407\n",
      "Epoch 36/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 32s 3ms/sample - loss: 0.6383 - acc: 0.8244\n",
      "834/834 [==============================] - 482s 578ms/step - loss: 0.1840 - acc: 0.9348 - val_loss: 0.6388 - val_acc: 0.8244\n",
      "Epoch 37/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.4109 - acc: 0.8733\n",
      "834/834 [==============================] - 489s 586ms/step - loss: 0.1851 - acc: 0.9349 - val_loss: 0.4114 - val_acc: 0.8733\n",
      "Epoch 38/200\n",
      "10000/10000 [==============================] - 32s 3ms/sample - loss: 0.5417 - acc: 0.8488\n",
      "834/834 [==============================] - 493s 592ms/step - loss: 0.1765 - acc: 0.9382 - val_loss: 0.5423 - val_acc: 0.8488\n",
      "Epoch 39/200\n",
      "10000/10000 [==============================] - 33s 3ms/sample - loss: 0.4320 - acc: 0.8721\n",
      "834/834 [==============================] - 496s 594ms/step - loss: 0.1743 - acc: 0.9382 - val_loss: 0.4324 - val_acc: 0.8721\n",
      "Epoch 40/200\n",
      "10000/10000 [==============================] - 32s 3ms/sample - loss: 0.4778 - acc: 0.8604\n",
      "834/834 [==============================] - 498s 597ms/step - loss: 0.1703 - acc: 0.9390 - val_loss: 0.4782 - val_acc: 0.8604\n",
      "Epoch 41/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.5225 - acc: 0.86360s - loss: 0.5215 - acc: 0.863\n",
      "834/834 [==============================] - 486s 583ms/step - loss: 0.1665 - acc: 0.9416 - val_loss: 0.5229 - val_acc: 0.8636\n",
      "Epoch 42/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.4806 - acc: 0.8642\n",
      "834/834 [==============================] - 483s 579ms/step - loss: 0.1630 - acc: 0.9416 - val_loss: 0.4814 - val_acc: 0.8642\n",
      "Epoch 43/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.4818 - acc: 0.8623\n",
      "834/834 [==============================] - 481s 577ms/step - loss: 0.1563 - acc: 0.9444 - val_loss: 0.4822 - val_acc: 0.8623\n",
      "Epoch 44/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.5034 - acc: 0.8605\n",
      "834/834 [==============================] - 480s 576ms/step - loss: 0.1550 - acc: 0.9442 - val_loss: 0.5040 - val_acc: 0.8605\n",
      "Epoch 45/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.4948 - acc: 0.8689\n",
      "834/834 [==============================] - 478s 573ms/step - loss: 0.1501 - acc: 0.9475 - val_loss: 0.4955 - val_acc: 0.8689\n",
      "Epoch 46/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.5386 - acc: 0.8556\n",
      "834/834 [==============================] - 478s 573ms/step - loss: 0.1511 - acc: 0.9474 - val_loss: 0.5386 - val_acc: 0.8556\n",
      "Epoch 47/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.5122 - acc: 0.8624\n",
      "834/834 [==============================] - 484s 580ms/step - loss: 0.1454 - acc: 0.9484 - val_loss: 0.5123 - val_acc: 0.8624\n",
      "Epoch 48/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.4817 - acc: 0.8680\n",
      "834/834 [==============================] - 488s 585ms/step - loss: 0.1466 - acc: 0.9476 - val_loss: 0.4822 - val_acc: 0.8680\n",
      "Epoch 49/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5050 - acc: 0.8641\n",
      "834/834 [==============================] - 483s 579ms/step - loss: 0.1396 - acc: 0.9504 - val_loss: 0.5057 - val_acc: 0.8641\n",
      "Epoch 50/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.4834 - acc: 0.8685\n",
      "834/834 [==============================] - 475s 570ms/step - loss: 0.1384 - acc: 0.9503 - val_loss: 0.4839 - val_acc: 0.8685\n",
      "Epoch 51/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.4766 - acc: 0.8660\n",
      "834/834 [==============================] - 476s 571ms/step - loss: 0.1359 - acc: 0.9520 - val_loss: 0.4766 - val_acc: 0.8660\n",
      "Epoch 52/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.4718 - acc: 0.874510s - ETA: 4s - loss: 0.471 - ETA: 2s - loss: 0.4756 - acc - ETA: 0s - loss: 0.4730 - acc: 0.\n",
      "834/834 [==============================] - 478s 573ms/step - loss: 0.1340 - acc: 0.9525 - val_loss: 0.4720 - val_acc: 0.8745\n",
      "Epoch 53/200\n",
      "10000/10000 [==============================] - 32s 3ms/sample - loss: 0.5459 - acc: 0.8579\n",
      "834/834 [==============================] - 480s 575ms/step - loss: 0.1285 - acc: 0.9547 - val_loss: 0.5460 - val_acc: 0.8579\n",
      "Epoch 54/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.4677 - acc: 0.8713\n",
      "834/834 [==============================] - 478s 573ms/step - loss: 0.1271 - acc: 0.9548 - val_loss: 0.4682 - val_acc: 0.8713\n",
      "Epoch 55/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.4871 - acc: 0.8711\n",
      "834/834 [==============================] - 477s 572ms/step - loss: 0.1258 - acc: 0.9555 - val_loss: 0.4871 - val_acc: 0.8711\n",
      "Epoch 56/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.4521 - acc: 0.8758\n",
      "834/834 [==============================] - 476s 571ms/step - loss: 0.1258 - acc: 0.9560 - val_loss: 0.4525 - val_acc: 0.8758\n",
      "Epoch 57/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.4420 - acc: 0.8817\n",
      "834/834 [==============================] - 477s 572ms/step - loss: 0.1222 - acc: 0.9564 - val_loss: 0.4426 - val_acc: 0.8817\n",
      "Epoch 58/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.4124 - acc: 0.8815\n",
      "834/834 [==============================] - 476s 571ms/step - loss: 0.1197 - acc: 0.9569 - val_loss: 0.4131 - val_acc: 0.8815\n",
      "Epoch 59/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.4801 - acc: 0.8749\n",
      "834/834 [==============================] - 476s 570ms/step - loss: 0.1159 - acc: 0.9590 - val_loss: 0.4807 - val_acc: 0.8749\n",
      "Epoch 60/200\n",
      "10000/10000 [==============================] - 32s 3ms/sample - loss: 0.5386 - acc: 0.8555\n",
      "834/834 [==============================] - 481s 577ms/step - loss: 0.1172 - acc: 0.9584 - val_loss: 0.5385 - val_acc: 0.8555\n",
      "Epoch 61/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.4748 - acc: 0.8778\n",
      "834/834 [==============================] - 471s 565ms/step - loss: 0.1141 - acc: 0.9601 - val_loss: 0.4754 - val_acc: 0.8778\n",
      "Epoch 62/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.4929 - acc: 0.8717\n",
      "834/834 [==============================] - 479s 575ms/step - loss: 0.1141 - acc: 0.9605 - val_loss: 0.4936 - val_acc: 0.8717\n",
      "Epoch 63/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.4725 - acc: 0.8788\n",
      "834/834 [==============================] - 477s 571ms/step - loss: 0.1073 - acc: 0.9606 - val_loss: 0.4730 - val_acc: 0.8788\n",
      "Epoch 64/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.6130 - acc: 0.8516\n",
      "834/834 [==============================] - 476s 571ms/step - loss: 0.1047 - acc: 0.9632 - val_loss: 0.6139 - val_acc: 0.8516\n",
      "Epoch 65/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.4945 - acc: 0.8760\n",
      "834/834 [==============================] - 478s 574ms/step - loss: 0.1123 - acc: 0.9602 - val_loss: 0.4948 - val_acc: 0.8760\n",
      "Epoch 66/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.4492 - acc: 0.8826\n",
      "834/834 [==============================] - 473s 567ms/step - loss: 0.1104 - acc: 0.9603 - val_loss: 0.4502 - val_acc: 0.8826\n",
      "Epoch 67/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5108 - acc: 0.8725\n",
      "834/834 [==============================] - 477s 572ms/step - loss: 0.0993 - acc: 0.9642 - val_loss: 0.5109 - val_acc: 0.8725\n",
      "Epoch 68/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.4773 - acc: 0.8763\n",
      "834/834 [==============================] - 478s 573ms/step - loss: 0.1017 - acc: 0.9638 - val_loss: 0.4778 - val_acc: 0.8763\n",
      "Epoch 69/200\n",
      "10000/10000 [==============================] - 29s 3ms/sample - loss: 0.4811 - acc: 0.8731\n",
      "834/834 [==============================] - 473s 568ms/step - loss: 0.1027 - acc: 0.9621 - val_loss: 0.4810 - val_acc: 0.8731\n",
      "Epoch 70/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.4897 - acc: 0.8758\n",
      "834/834 [==============================] - 471s 565ms/step - loss: 0.1011 - acc: 0.9649 - val_loss: 0.4897 - val_acc: 0.8758\n",
      "Epoch 71/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.4807 - acc: 0.8790\n",
      "834/834 [==============================] - 470s 564ms/step - loss: 0.0957 - acc: 0.9661 - val_loss: 0.4810 - val_acc: 0.8790\n",
      "Epoch 72/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.4896 - acc: 0.8804\n",
      "834/834 [==============================] - 473s 567ms/step - loss: 0.0996 - acc: 0.9648 - val_loss: 0.4896 - val_acc: 0.8804\n",
      "Epoch 73/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.4724 - acc: 0.8785\n",
      "834/834 [==============================] - 471s 564ms/step - loss: 0.0965 - acc: 0.9657 - val_loss: 0.4722 - val_acc: 0.8785\n",
      "Epoch 74/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5481 - acc: 0.8689\n",
      "834/834 [==============================] - 469s 562ms/step - loss: 0.0933 - acc: 0.9676 - val_loss: 0.5477 - val_acc: 0.8689\n",
      "Epoch 75/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5854 - acc: 0.8680\n",
      "834/834 [==============================] - 469s 562ms/step - loss: 0.0909 - acc: 0.9677 - val_loss: 0.5860 - val_acc: 0.8680\n",
      "Epoch 76/200\n",
      "10000/10000 [==============================] - 29s 3ms/sample - loss: 0.5507 - acc: 0.8739\n",
      "834/834 [==============================] - 464s 556ms/step - loss: 0.0905 - acc: 0.9679 - val_loss: 0.5514 - val_acc: 0.8739\n",
      "Epoch 77/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.4472 - acc: 0.8808\n",
      "834/834 [==============================] - 467s 560ms/step - loss: 0.0902 - acc: 0.9671 - val_loss: 0.4474 - val_acc: 0.8808\n",
      "Epoch 78/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5737 - acc: 0.8641\n",
      "834/834 [==============================] - 470s 564ms/step - loss: 0.0888 - acc: 0.9685 - val_loss: 0.5742 - val_acc: 0.8641\n",
      "Epoch 79/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5309 - acc: 0.8758\n",
      "834/834 [==============================] - 475s 570ms/step - loss: 0.0913 - acc: 0.9673 - val_loss: 0.5319 - val_acc: 0.8758\n",
      "Epoch 80/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5261 - acc: 0.8779\n",
      "834/834 [==============================] - 476s 570ms/step - loss: 0.0883 - acc: 0.9690 - val_loss: 0.5266 - val_acc: 0.8779\n",
      "Epoch 81/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5856 - acc: 0.8674\n",
      "834/834 [==============================] - 475s 569ms/step - loss: 0.0855 - acc: 0.9692 - val_loss: 0.5861 - val_acc: 0.8674\n",
      "Epoch 82/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.4893 - acc: 0.8834\n",
      "834/834 [==============================] - 475s 569ms/step - loss: 0.0869 - acc: 0.9698 - val_loss: 0.4905 - val_acc: 0.8834\n",
      "Epoch 83/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.4965 - acc: 0.8821\n",
      "834/834 [==============================] - 478s 574ms/step - loss: 0.0846 - acc: 0.9694 - val_loss: 0.4968 - val_acc: 0.8821\n",
      "Epoch 84/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.4882 - acc: 0.8830\n",
      "834/834 [==============================] - 486s 583ms/step - loss: 0.0848 - acc: 0.9699 - val_loss: 0.4892 - val_acc: 0.8830\n",
      "Epoch 85/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.5922 - acc: 0.8655\n",
      "834/834 [==============================] - 484s 580ms/step - loss: 0.0819 - acc: 0.9708 - val_loss: 0.5920 - val_acc: 0.8655\n",
      "Epoch 86/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.5571 - acc: 0.8734\n",
      "834/834 [==============================] - 484s 581ms/step - loss: 0.0811 - acc: 0.9708 - val_loss: 0.5573 - val_acc: 0.8734\n",
      "Epoch 87/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.6007 - acc: 0.8627\n",
      "834/834 [==============================] - 484s 580ms/step - loss: 0.0809 - acc: 0.9715 - val_loss: 0.6012 - val_acc: 0.8627\n",
      "Epoch 88/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.4840 - acc: 0.8880\n",
      "834/834 [==============================] - 485s 582ms/step - loss: 0.0794 - acc: 0.9727 - val_loss: 0.4848 - val_acc: 0.8880\n",
      "Epoch 89/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.5115 - acc: 0.88186s - loss:  - ETA: 3s - loss: 0.\n",
      "834/834 [==============================] - 483s 580ms/step - loss: 0.0795 - acc: 0.9718 - val_loss: 0.5120 - val_acc: 0.8818\n",
      "Epoch 90/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.4863 - acc: 0.88910s - loss: 0.4873 - acc: 0.88\n",
      "834/834 [==============================] - 482s 578ms/step - loss: 0.0760 - acc: 0.9734 - val_loss: 0.4863 - val_acc: 0.8891\n",
      "Epoch 91/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.5387 - acc: 0.8740\n",
      "834/834 [==============================] - 487s 584ms/step - loss: 0.0799 - acc: 0.9718 - val_loss: 0.5391 - val_acc: 0.8740\n",
      "Epoch 92/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.5967 - acc: 0.86461s - loss: 0.5981 - acc: \n",
      "834/834 [==============================] - 484s 581ms/step - loss: 0.0753 - acc: 0.9732 - val_loss: 0.5980 - val_acc: 0.8646\n",
      "Epoch 93/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.5567 - acc: 0.8738\n",
      "834/834 [==============================] - 481s 577ms/step - loss: 0.0788 - acc: 0.9714 - val_loss: 0.5567 - val_acc: 0.8738\n",
      "Epoch 94/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.4814 - acc: 0.8871\n",
      "834/834 [==============================] - 482s 578ms/step - loss: 0.0753 - acc: 0.9730 - val_loss: 0.4817 - val_acc: 0.8871\n",
      "Epoch 95/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.5195 - acc: 0.8832\n",
      "834/834 [==============================] - 484s 581ms/step - loss: 0.0725 - acc: 0.9749 - val_loss: 0.5201 - val_acc: 0.8832\n",
      "Epoch 96/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.5195 - acc: 0.8824\n",
      "834/834 [==============================] - 481s 577ms/step - loss: 0.0729 - acc: 0.9741 - val_loss: 0.5198 - val_acc: 0.8824\n",
      "Epoch 97/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.5481 - acc: 0.87612s - loss: 0.5501 \n",
      "834/834 [==============================] - 480s 576ms/step - loss: 0.0756 - acc: 0.9734 - val_loss: 0.5494 - val_acc: 0.8761\n",
      "Epoch 98/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.5205 - acc: 0.8832\n",
      "834/834 [==============================] - 482s 577ms/step - loss: 0.0673 - acc: 0.9761 - val_loss: 0.5207 - val_acc: 0.8832\n",
      "Epoch 99/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.5195 - acc: 0.8847\n",
      "834/834 [==============================] - 485s 582ms/step - loss: 0.0719 - acc: 0.9749 - val_loss: 0.5199 - val_acc: 0.8847\n",
      "Epoch 100/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.5595 - acc: 0.8764\n",
      "834/834 [==============================] - 481s 576ms/step - loss: 0.0688 - acc: 0.9758 - val_loss: 0.5597 - val_acc: 0.8764\n",
      "Epoch 101/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.5404 - acc: 0.87973s - loss: 0.5\n",
      "834/834 [==============================] - 481s 576ms/step - loss: 0.0705 - acc: 0.9755 - val_loss: 0.5412 - val_acc: 0.8797\n",
      "Epoch 102/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.5498 - acc: 0.8801\n",
      "834/834 [==============================] - 481s 576ms/step - loss: 0.0693 - acc: 0.9758 - val_loss: 0.5498 - val_acc: 0.8801\n",
      "Epoch 103/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.5212 - acc: 0.8859\n",
      "834/834 [==============================] - 483s 579ms/step - loss: 0.0674 - acc: 0.9762 - val_loss: 0.5219 - val_acc: 0.8859\n",
      "Epoch 104/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.6052 - acc: 0.8737\n",
      "834/834 [==============================] - 483s 579ms/step - loss: 0.0661 - acc: 0.9766 - val_loss: 0.6057 - val_acc: 0.8737\n",
      "Epoch 105/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.5651 - acc: 0.87883s - loss: \n",
      "834/834 [==============================] - 480s 576ms/step - loss: 0.0669 - acc: 0.9766 - val_loss: 0.5660 - val_acc: 0.8788\n",
      "Epoch 106/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.5283 - acc: 0.8872\n",
      "834/834 [==============================] - 479s 575ms/step - loss: 0.0656 - acc: 0.9758 - val_loss: 0.5289 - val_acc: 0.8872\n",
      "Epoch 107/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.7342 - acc: 0.8538\n",
      "834/834 [==============================] - 481s 577ms/step - loss: 0.0647 - acc: 0.9769 - val_loss: 0.7351 - val_acc: 0.8538\n",
      "Epoch 108/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.6231 - acc: 0.8707\n",
      "834/834 [==============================] - 480s 575ms/step - loss: 0.0628 - acc: 0.9773 - val_loss: 0.6237 - val_acc: 0.8707\n",
      "Epoch 109/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.5415 - acc: 0.8821\n",
      "834/834 [==============================] - 479s 574ms/step - loss: 0.0638 - acc: 0.9778 - val_loss: 0.5436 - val_acc: 0.8821\n",
      "Epoch 110/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.5349 - acc: 0.8787\n",
      "834/834 [==============================] - 480s 576ms/step - loss: 0.0678 - acc: 0.9761 - val_loss: 0.5356 - val_acc: 0.8787\n",
      "Epoch 111/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.5273 - acc: 0.8872\n",
      "834/834 [==============================] - 482s 578ms/step - loss: 0.0620 - acc: 0.9780 - val_loss: 0.5274 - val_acc: 0.8872\n",
      "Epoch 112/200\n",
      "10000/10000 [==============================] - 32s 3ms/sample - loss: 0.5520 - acc: 0.8831\n",
      "834/834 [==============================] - 484s 580ms/step - loss: 0.0633 - acc: 0.9775 - val_loss: 0.5519 - val_acc: 0.8831\n",
      "Epoch 113/200\n",
      "10000/10000 [==============================] - 32s 3ms/sample - loss: 0.6407 - acc: 0.8693\n",
      "834/834 [==============================] - 491s 588ms/step - loss: 0.0629 - acc: 0.9783 - val_loss: 0.6416 - val_acc: 0.8693\n",
      "Epoch 114/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.5496 - acc: 0.8806\n",
      "834/834 [==============================] - 489s 587ms/step - loss: 0.0622 - acc: 0.9780 - val_loss: 0.5508 - val_acc: 0.8806\n",
      "Epoch 115/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.6007 - acc: 0.8734\n",
      "834/834 [==============================] - 483s 580ms/step - loss: 0.0647 - acc: 0.9775 - val_loss: 0.6010 - val_acc: 0.8734\n",
      "Epoch 116/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.5154 - acc: 0.8871\n",
      "834/834 [==============================] - 481s 576ms/step - loss: 0.0593 - acc: 0.9796 - val_loss: 0.5161 - val_acc: 0.8871\n",
      "Epoch 117/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.6489 - acc: 0.8675\n",
      "834/834 [==============================] - 479s 575ms/step - loss: 0.0605 - acc: 0.9781 - val_loss: 0.6491 - val_acc: 0.8675\n",
      "Epoch 118/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.5772 - acc: 0.8741\n",
      "834/834 [==============================] - 479s 575ms/step - loss: 0.0609 - acc: 0.9782 - val_loss: 0.5784 - val_acc: 0.8741\n",
      "Epoch 119/200\n",
      "10000/10000 [==============================] - 32s 3ms/sample - loss: 0.5835 - acc: 0.8716\n",
      "834/834 [==============================] - 496s 594ms/step - loss: 0.0600 - acc: 0.9791 - val_loss: 0.5844 - val_acc: 0.8716\n",
      "Epoch 120/200\n",
      "10000/10000 [==============================] - 32s 3ms/sample - loss: 0.5768 - acc: 0.8791\n",
      "834/834 [==============================] - 492s 590ms/step - loss: 0.0584 - acc: 0.9790 - val_loss: 0.5771 - val_acc: 0.8791\n",
      "Epoch 121/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5301 - acc: 0.8834\n",
      "834/834 [==============================] - 484s 580ms/step - loss: 0.0590 - acc: 0.9796 - val_loss: 0.5310 - val_acc: 0.8834\n",
      "Epoch 122/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5776 - acc: 0.8737\n",
      "834/834 [==============================] - 476s 571ms/step - loss: 0.0621 - acc: 0.9778 - val_loss: 0.5784 - val_acc: 0.8737\n",
      "Epoch 123/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5128 - acc: 0.8878\n",
      "834/834 [==============================] - 477s 571ms/step - loss: 0.0578 - acc: 0.9798 - val_loss: 0.5135 - val_acc: 0.8878\n",
      "Epoch 124/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5371 - acc: 0.8821\n",
      "834/834 [==============================] - 476s 571ms/step - loss: 0.0590 - acc: 0.9802 - val_loss: 0.5376 - val_acc: 0.8821\n",
      "Epoch 125/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5250 - acc: 0.8854\n",
      "834/834 [==============================] - 474s 568ms/step - loss: 0.0569 - acc: 0.9804 - val_loss: 0.5258 - val_acc: 0.8854\n",
      "Epoch 126/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.6675 - acc: 0.8679\n",
      "834/834 [==============================] - 474s 569ms/step - loss: 0.0562 - acc: 0.9803 - val_loss: 0.6696 - val_acc: 0.8679\n",
      "Epoch 127/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5188 - acc: 0.8880\n",
      "834/834 [==============================] - 478s 573ms/step - loss: 0.0541 - acc: 0.9804 - val_loss: 0.5202 - val_acc: 0.8880\n",
      "Epoch 128/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5260 - acc: 0.8853\n",
      "834/834 [==============================] - 475s 570ms/step - loss: 0.0601 - acc: 0.9790 - val_loss: 0.5266 - val_acc: 0.8853\n",
      "Epoch 129/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5574 - acc: 0.8816\n",
      "834/834 [==============================] - 474s 568ms/step - loss: 0.0591 - acc: 0.9786 - val_loss: 0.5582 - val_acc: 0.8816\n",
      "Epoch 130/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5359 - acc: 0.8877\n",
      "834/834 [==============================] - 474s 568ms/step - loss: 0.0556 - acc: 0.9805 - val_loss: 0.5360 - val_acc: 0.8877\n",
      "Epoch 131/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5275 - acc: 0.8870\n",
      "834/834 [==============================] - 476s 570ms/step - loss: 0.0498 - acc: 0.9829 - val_loss: 0.5279 - val_acc: 0.8870\n",
      "Epoch 132/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5682 - acc: 0.88340s - loss: 0.5682 - acc: 0.8\n",
      "834/834 [==============================] - 477s 572ms/step - loss: 0.0550 - acc: 0.9808 - val_loss: 0.5685 - val_acc: 0.8834\n",
      "Epoch 133/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5263 - acc: 0.8893\n",
      "834/834 [==============================] - 474s 569ms/step - loss: 0.0541 - acc: 0.9814 - val_loss: 0.5269 - val_acc: 0.8893\n",
      "Epoch 134/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5840 - acc: 0.88190s - loss: 0.5855 - acc: 0.8\n",
      "834/834 [==============================] - 475s 569ms/step - loss: 0.0514 - acc: 0.9823 - val_loss: 0.5845 - val_acc: 0.8819\n",
      "Epoch 135/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5730 - acc: 0.8814\n",
      "834/834 [==============================] - 476s 571ms/step - loss: 0.0533 - acc: 0.9815 - val_loss: 0.5734 - val_acc: 0.8814\n",
      "Epoch 136/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5965 - acc: 0.8786\n",
      "834/834 [==============================] - 474s 569ms/step - loss: 0.0491 - acc: 0.9825 - val_loss: 0.5976 - val_acc: 0.8786\n",
      "Epoch 137/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5453 - acc: 0.8848\n",
      "834/834 [==============================] - 473s 567ms/step - loss: 0.0520 - acc: 0.9818 - val_loss: 0.5460 - val_acc: 0.8848\n",
      "Epoch 138/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5328 - acc: 0.88891s - loss: 0.5370 - a\n",
      "834/834 [==============================] - 474s 569ms/step - loss: 0.0547 - acc: 0.9813 - val_loss: 0.5331 - val_acc: 0.8889\n",
      "Epoch 139/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5150 - acc: 0.8896\n",
      "834/834 [==============================] - 475s 570ms/step - loss: 0.0514 - acc: 0.9821 - val_loss: 0.5158 - val_acc: 0.8896\n",
      "Epoch 140/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5822 - acc: 0.8824\n",
      "834/834 [==============================] - 470s 564ms/step - loss: 0.0500 - acc: 0.9827 - val_loss: 0.5830 - val_acc: 0.8824\n",
      "Epoch 141/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5333 - acc: 0.8898\n",
      "834/834 [==============================] - 468s 561ms/step - loss: 0.0536 - acc: 0.9815 - val_loss: 0.5342 - val_acc: 0.8898\n",
      "Epoch 142/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5492 - acc: 0.8856\n",
      "834/834 [==============================] - 467s 560ms/step - loss: 0.0511 - acc: 0.9823 - val_loss: 0.5499 - val_acc: 0.8856\n",
      "Epoch 143/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5567 - acc: 0.8908\n",
      "834/834 [==============================] - 469s 562ms/step - loss: 0.0501 - acc: 0.9820 - val_loss: 0.5575 - val_acc: 0.8908\n",
      "Epoch 144/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5787 - acc: 0.8777\n",
      "834/834 [==============================] - 470s 564ms/step - loss: 0.0531 - acc: 0.9810 - val_loss: 0.5798 - val_acc: 0.8777\n",
      "Epoch 145/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5328 - acc: 0.89023s - loss:\n",
      "834/834 [==============================] - 467s 560ms/step - loss: 0.0502 - acc: 0.9818 - val_loss: 0.5335 - val_acc: 0.8902\n",
      "Epoch 146/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.6252 - acc: 0.8767\n",
      "834/834 [==============================] - 468s 561ms/step - loss: 0.0460 - acc: 0.9839 - val_loss: 0.6268 - val_acc: 0.8767\n",
      "Epoch 147/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.6151 - acc: 0.8817\n",
      "834/834 [==============================] - 469s 562ms/step - loss: 0.0516 - acc: 0.9820 - val_loss: 0.6168 - val_acc: 0.8817\n",
      "Epoch 148/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.6186 - acc: 0.8804\n",
      "834/834 [==============================] - 470s 564ms/step - loss: 0.0485 - acc: 0.9833 - val_loss: 0.6197 - val_acc: 0.8804\n",
      "Epoch 149/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5637 - acc: 0.8889\n",
      "834/834 [==============================] - 467s 560ms/step - loss: 0.0485 - acc: 0.9833 - val_loss: 0.5647 - val_acc: 0.8889\n",
      "Epoch 150/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.6387 - acc: 0.8724\n",
      "834/834 [==============================] - 469s 563ms/step - loss: 0.0474 - acc: 0.9834 - val_loss: 0.6394 - val_acc: 0.8724\n",
      "Epoch 151/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5410 - acc: 0.8842\n",
      "834/834 [==============================] - 471s 565ms/step - loss: 0.0479 - acc: 0.9834 - val_loss: 0.5420 - val_acc: 0.8842\n",
      "Epoch 152/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5122 - acc: 0.8922\n",
      "834/834 [==============================] - 468s 561ms/step - loss: 0.0503 - acc: 0.9825 - val_loss: 0.5132 - val_acc: 0.8922\n",
      "Epoch 153/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.6354 - acc: 0.87583s - loss: 0.\n",
      "834/834 [==============================] - 466s 559ms/step - loss: 0.0445 - acc: 0.9841 - val_loss: 0.6366 - val_acc: 0.8758\n",
      "Epoch 154/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.6555 - acc: 0.8739\n",
      "834/834 [==============================] - 468s 561ms/step - loss: 0.0444 - acc: 0.9842 - val_loss: 0.6568 - val_acc: 0.8739\n",
      "Epoch 155/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5127 - acc: 0.8903\n",
      "834/834 [==============================] - 469s 563ms/step - loss: 0.0482 - acc: 0.9834 - val_loss: 0.5131 - val_acc: 0.8903\n",
      "Epoch 156/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.6535 - acc: 0.8786\n",
      "834/834 [==============================] - 470s 563ms/step - loss: 0.0501 - acc: 0.9829 - val_loss: 0.6538 - val_acc: 0.8786\n",
      "Epoch 157/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5971 - acc: 0.881710s - los\n",
      "834/834 [==============================] - 468s 561ms/step - loss: 0.0439 - acc: 0.9840 - val_loss: 0.5976 - val_acc: 0.8817\n",
      "Epoch 158/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5744 - acc: 0.8837\n",
      "834/834 [==============================] - 469s 562ms/step - loss: 0.0447 - acc: 0.9838 - val_loss: 0.5743 - val_acc: 0.8837\n",
      "Epoch 159/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5644 - acc: 0.88910s - loss: 0.5652 - acc: 0\n",
      "834/834 [==============================] - 466s 558ms/step - loss: 0.0446 - acc: 0.9843 - val_loss: 0.5655 - val_acc: 0.8891\n",
      "Epoch 160/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5232 - acc: 0.8921\n",
      "834/834 [==============================] - 466s 559ms/step - loss: 0.0464 - acc: 0.9839 - val_loss: 0.5239 - val_acc: 0.8921\n",
      "Epoch 161/200\n",
      "10000/10000 [==============================] - 32s 3ms/sample - loss: 0.6407 - acc: 0.8786\n",
      "834/834 [==============================] - 478s 573ms/step - loss: 0.0458 - acc: 0.9840 - val_loss: 0.6417 - val_acc: 0.8786\n",
      "Epoch 162/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.6017 - acc: 0.8851\n",
      "834/834 [==============================] - 479s 574ms/step - loss: 0.0417 - acc: 0.9852 - val_loss: 0.6040 - val_acc: 0.8851\n",
      "Epoch 163/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.5643 - acc: 0.8930\n",
      "834/834 [==============================] - 482s 577ms/step - loss: 0.0427 - acc: 0.9853 - val_loss: 0.5655 - val_acc: 0.8930\n",
      "Epoch 164/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.5948 - acc: 0.8849\n",
      "834/834 [==============================] - 486s 583ms/step - loss: 0.0467 - acc: 0.9837 - val_loss: 0.5959 - val_acc: 0.8849\n",
      "Epoch 165/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5588 - acc: 0.88701s - loss: 0.5637 - acc\n",
      "834/834 [==============================] - 473s 567ms/step - loss: 0.0445 - acc: 0.9841 - val_loss: 0.5593 - val_acc: 0.8870\n",
      "Epoch 166/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5356 - acc: 0.8933\n",
      "834/834 [==============================] - 469s 563ms/step - loss: 0.0427 - acc: 0.9852 - val_loss: 0.5362 - val_acc: 0.8933\n",
      "Epoch 167/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.6431 - acc: 0.88122s - loss: 0.647\n",
      "834/834 [==============================] - 468s 561ms/step - loss: 0.0451 - acc: 0.9846 - val_loss: 0.6437 - val_acc: 0.8812\n",
      "Epoch 168/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5264 - acc: 0.8904\n",
      "834/834 [==============================] - 468s 561ms/step - loss: 0.0422 - acc: 0.9851 - val_loss: 0.5267 - val_acc: 0.8904\n",
      "Epoch 169/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5656 - acc: 0.8901\n",
      "834/834 [==============================] - 465s 558ms/step - loss: 0.0429 - acc: 0.9851 - val_loss: 0.5667 - val_acc: 0.8901\n",
      "Epoch 170/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.6155 - acc: 0.8809\n",
      "834/834 [==============================] - 468s 561ms/step - loss: 0.0439 - acc: 0.9844 - val_loss: 0.6164 - val_acc: 0.8809\n",
      "Epoch 171/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5674 - acc: 0.8864\n",
      "834/834 [==============================] - 474s 568ms/step - loss: 0.0431 - acc: 0.9844 - val_loss: 0.5675 - val_acc: 0.8864\n",
      "Epoch 172/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.6003 - acc: 0.8839\n",
      "834/834 [==============================] - 474s 568ms/step - loss: 0.0419 - acc: 0.9852 - val_loss: 0.6014 - val_acc: 0.8839\n",
      "Epoch 173/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5629 - acc: 0.8911\n",
      "834/834 [==============================] - 467s 561ms/step - loss: 0.0433 - acc: 0.9847 - val_loss: 0.5641 - val_acc: 0.8911\n",
      "Epoch 174/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5339 - acc: 0.8935\n",
      "834/834 [==============================] - 466s 559ms/step - loss: 0.0398 - acc: 0.9859 - val_loss: 0.5342 - val_acc: 0.8935\n",
      "Epoch 175/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.5547 - acc: 0.8901\n",
      "834/834 [==============================] - 472s 565ms/step - loss: 0.0445 - acc: 0.9843 - val_loss: 0.5555 - val_acc: 0.8901\n",
      "Epoch 176/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5982 - acc: 0.8878\n",
      "834/834 [==============================] - 474s 569ms/step - loss: 0.0408 - acc: 0.9856 - val_loss: 0.5991 - val_acc: 0.8878\n",
      "Epoch 177/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5808 - acc: 0.8883\n",
      "834/834 [==============================] - 470s 563ms/step - loss: 0.0391 - acc: 0.9861 - val_loss: 0.5823 - val_acc: 0.8883\n",
      "Epoch 178/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5623 - acc: 0.8889\n",
      "834/834 [==============================] - 470s 564ms/step - loss: 0.0434 - acc: 0.9853 - val_loss: 0.5634 - val_acc: 0.8889\n",
      "Epoch 179/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.6388 - acc: 0.8773\n",
      "834/834 [==============================] - 471s 564ms/step - loss: 0.0421 - acc: 0.9853 - val_loss: 0.6386 - val_acc: 0.8773\n",
      "Epoch 180/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.6809 - acc: 0.8739\n",
      "834/834 [==============================] - 472s 566ms/step - loss: 0.0430 - acc: 0.9846 - val_loss: 0.6819 - val_acc: 0.8739\n",
      "Epoch 181/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5757 - acc: 0.8883\n",
      "834/834 [==============================] - 469s 563ms/step - loss: 0.0412 - acc: 0.9855 - val_loss: 0.5768 - val_acc: 0.8883\n",
      "Epoch 182/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5634 - acc: 0.8918\n",
      "834/834 [==============================] - 466s 558ms/step - loss: 0.0407 - acc: 0.9856 - val_loss: 0.5637 - val_acc: 0.8918\n",
      "Epoch 183/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.6496 - acc: 0.8784\n",
      "834/834 [==============================] - 467s 560ms/step - loss: 0.0407 - acc: 0.9862 - val_loss: 0.6493 - val_acc: 0.8784\n",
      "Epoch 184/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5490 - acc: 0.8924\n",
      "834/834 [==============================] - 467s 560ms/step - loss: 0.0416 - acc: 0.9857 - val_loss: 0.5489 - val_acc: 0.8924\n",
      "Epoch 185/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.6042 - acc: 0.88873s - loss: 0.6\n",
      "834/834 [==============================] - 472s 566ms/step - loss: 0.0386 - acc: 0.9862 - val_loss: 0.6051 - val_acc: 0.8887\n",
      "Epoch 186/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5888 - acc: 0.8876\n",
      "834/834 [==============================] - 474s 568ms/step - loss: 0.0409 - acc: 0.9857 - val_loss: 0.5893 - val_acc: 0.8876\n",
      "Epoch 187/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5713 - acc: 0.8882\n",
      "834/834 [==============================] - 474s 568ms/step - loss: 0.0399 - acc: 0.9860 - val_loss: 0.5719 - val_acc: 0.8882\n",
      "Epoch 188/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.7406 - acc: 0.8670\n",
      "834/834 [==============================] - 475s 569ms/step - loss: 0.0392 - acc: 0.9858 - val_loss: 0.7429 - val_acc: 0.8670\n",
      "Epoch 189/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5844 - acc: 0.89031s - loss: 0.5834 - acc:\n",
      "834/834 [==============================] - 472s 566ms/step - loss: 0.0383 - acc: 0.9865 - val_loss: 0.5852 - val_acc: 0.8903\n",
      "Epoch 190/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.6447 - acc: 0.88501s - loss: 0.6472 - acc\n",
      "834/834 [==============================] - 471s 564ms/step - loss: 0.0399 - acc: 0.9860 - val_loss: 0.6459 - val_acc: 0.8850\n",
      "Epoch 191/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5784 - acc: 0.8886\n",
      "834/834 [==============================] - 473s 568ms/step - loss: 0.0366 - acc: 0.9874 - val_loss: 0.5781 - val_acc: 0.8886\n",
      "Epoch 192/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5735 - acc: 0.8924\n",
      "834/834 [==============================] - 474s 568ms/step - loss: 0.0411 - acc: 0.9856 - val_loss: 0.5742 - val_acc: 0.8924\n",
      "Epoch 193/200\n",
      "10000/10000 [==============================] - 31s 3ms/sample - loss: 0.5691 - acc: 0.8895\n",
      "834/834 [==============================] - 468s 561ms/step - loss: 0.0370 - acc: 0.9871 - val_loss: 0.5698 - val_acc: 0.8895\n",
      "Epoch 194/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5587 - acc: 0.89204s -\n",
      "834/834 [==============================] - 469s 562ms/step - loss: 0.0394 - acc: 0.9865 - val_loss: 0.5591 - val_acc: 0.8920\n",
      "Epoch 195/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5578 - acc: 0.8902\n",
      "834/834 [==============================] - 474s 568ms/step - loss: 0.0386 - acc: 0.9863 - val_loss: 0.5577 - val_acc: 0.8902\n",
      "Epoch 196/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5446 - acc: 0.89633s - loss: \n",
      "834/834 [==============================] - 473s 567ms/step - loss: 0.0381 - acc: 0.9867 - val_loss: 0.5459 - val_acc: 0.8963\n",
      "Epoch 197/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.6233 - acc: 0.8852\n",
      "834/834 [==============================] - 467s 560ms/step - loss: 0.0377 - acc: 0.9864 - val_loss: 0.6243 - val_acc: 0.8852\n",
      "Epoch 198/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5495 - acc: 0.8933\n",
      "834/834 [==============================] - 472s 566ms/step - loss: 0.0408 - acc: 0.9858 - val_loss: 0.5501 - val_acc: 0.8933\n",
      "Epoch 199/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5552 - acc: 0.89240s - loss: 0.5539 - acc: 0.892\n",
      "834/834 [==============================] - 478s 574ms/step - loss: 0.0373 - acc: 0.9873 - val_loss: 0.5558 - val_acc: 0.8924\n",
      "Epoch 200/200\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5935 - acc: 0.8873\n",
      "834/834 [==============================] - 475s 569ms/step - loss: 0.0330 - acc: 0.9884 - val_loss: 0.5940 - val_acc: 0.8873\n",
      "> 88.730\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def summarize_diagnostics(history):\n",
    "\t# plot loss\n",
    "\tpyplot.subplot(121)\n",
    "\tpyplot.title('Cross Entropy Loss')\n",
    "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "\tpyplot.show()\n",
    "\t\n",
    " \n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t\n",
    "\t# define model\n",
    "\t\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "\t# prepare iterator\n",
    "\tit_train = datagen.flow(X_train, y_train, batch_size=60)\n",
    "\t# fit model\n",
    "\tsteps = int(X_train.shape[0] / 39)\n",
    "\thistory = model.fit_generator(it_train, steps_per_epoch=steps, epochs=200, validation_data=(X_test, y_test), verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# learning curves\n",
    "\tsummarize_diagnostics(history)\n",
    " \n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "#saving model weights \n",
    "model.save('my_model.h5') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ###### JUST CONTINUING THE MODEL FOR ANOTHER 5 EPOCHS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "colab_type": "code",
    "id": "P4ZIxDdaMXEK",
    "outputId": "81bbfd2a-fb72-4971-b7b5-f362ca122a71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5500 - acc: 0.8941\n",
      "834/834 [==============================] - 462s 554ms/step - loss: 0.0268 - acc: 0.9910 - val_loss: 0.5499 - val_acc: 0.8941\n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.6256 - acc: 0.8842\n",
      "834/834 [==============================] - 472s 565ms/step - loss: 0.0319 - acc: 0.9892 - val_loss: 0.6264 - val_acc: 0.8842\n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.6039 - acc: 0.8868\n",
      "834/834 [==============================] - 472s 566ms/step - loss: 0.0362 - acc: 0.9874 - val_loss: 0.6047 - val_acc: 0.8868\n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5519 - acc: 0.8920\n",
      "834/834 [==============================] - 467s 559ms/step - loss: 0.0406 - acc: 0.9857 - val_loss: 0.5532 - val_acc: 0.8920\n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 30s 3ms/sample - loss: 0.5623 - acc: 0.8954\n",
      "834/834 [==============================] - 468s 561ms/step - loss: 0.0350 - acc: 0.9877 - val_loss: 0.5632 - val_acc: 0.8954\n",
      "> 89.540\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def summarize_diagnostics(history):\n",
    "\t# plot loss\n",
    "\tpyplot.subplot(121)\n",
    "\tpyplot.title('Cross Entropy Loss')\n",
    "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "\tpyplot.show()\n",
    "\t\n",
    " \n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t\n",
    "\t# define model\n",
    "\t\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "\t# prepare iterator\n",
    "\tit_train = datagen.flow(X_train, y_train, batch_size=60)\n",
    "\t# fit model\n",
    "\tsteps = int(X_train.shape[0] / 39)\n",
    "\thistory = model.fit_generator(it_train, steps_per_epoch=steps, epochs=5, validation_data=(X_test, y_test), verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# learning curves\n",
    "\tsummarize_diagnostics(history)\n",
    " \n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_loss: 0.0350               train_acc: 0.9877 \n",
    "        \n",
    "val_loss: 0.5632       val_acc: 0.8954"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+--------+------------+-----------+-----------+----------+\n",
      "|           Model           | epochs | train_loss | train acc | test loss | test acc |\n",
      "+---------------------------+--------+------------+-----------+-----------+----------+\n",
      "|   model with dense layer  |   75   |   0.1203   |   0.956   |   0.6271  |  0.843   |\n",
      "| model without dense layer |  205   |   0.035    |   0.987   |   0.563   |  0.895   |\n",
      "+---------------------------+--------+------------+-----------+-----------+----------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "conclusion= PrettyTable()\n",
    "conclusion.field_names = [ \"Model\", 'epochs','train_loss','train acc',\"test loss\",'test acc']\n",
    "\n",
    "conclusion.add_row([\"model with dense layer\", 75,0.1203, 0.956, 0.6271,0.843])\n",
    "conclusion.add_row([\"model without dense layer\",205, 0.035, 0.987,0.563,0.895])\n",
    "\n",
    "\n",
    "print(conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "\n",
    "> Overfitting is one the problem in this assignment since dropouts was excluded.\n",
    "\n",
    "> Even tried using L2 regularization but still models are overfitting.\n",
    "\n",
    "> Loss on test data doesnot change after certain number of iterations which can be seen from plots.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Untitled7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
